{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jijijihioka\n",
      "covid\n",
      "virus\n",
      "viruz\n",
      "birus\n",
      "biruz\n",
      "sars cov\n",
      "sars-cov\n",
      "sarscov\n",
      "contingencia\n",
      "sanitaria\n",
      "sintoma\n",
      "síntoma\n",
      "neumonia\n",
      "neumonía\n",
      "quedateencasa\n",
      "quedate en casa\n",
      "pandemia\n",
      "encierro\n",
      "cuarentena\n",
      "encasa\n",
      "cuandoestoseacabe\n",
      "aislamientosocial\n",
      "susana distancia\n",
      "susanadistancia\n",
      "pfizer\n",
      "biontech\n",
      "vacuna\n",
      "vacunas\n",
      "moderna\n",
      "vacunacion\n",
      "vacunación\n",
      "vacúnate\n",
      "vacunate\n",
      "vacunenme\n",
      "vacúnenme\n",
      "astrazeneca\n",
      "cansino\n",
      "cancino\n",
      "astra zeneca\n",
      "CODVID19\n",
      "coronavirus\n",
      "covid19\n",
      "Covid\n",
      "COVID~19\n",
      "COVID-19\n",
      "Covid-19\n",
      "Coronavirus\n",
      "neumonía atípica\n",
      "SARS-CoV2\n",
      "virusito\n",
      "COVID 19\n",
      "Covid\n",
      "pseudocuarentena\n",
      "cuarentena\n",
      "pandemia\n",
      "crisis\n",
      "SARS-CoV 2\n",
      "#BastaDeFakeNews\n",
      "#CODVID19\n",
      "#QuedateEnCasa\n",
      "#UltimaOportunidad\n",
      "#TecuidasTúNosCuidamosTodos\n",
      "#COVID19mx\n",
      "#Covid_19\n",
      "#QuédateEnTuCasa\n",
      "#SusanaDistancia\n",
      "#stayhome\n",
      "#StayAtHomeAndStaySafe\n",
      "#COVID19mexico\n",
      "#ConferenciaCovid19\n",
      "#StayHome\n",
      "#QuédateEnCasa\n",
      "#COVID2019mx\n",
      "#carroñavirus\n",
      "#encasa\n",
      "#MeQuedoEnCasa\n",
      "#CuarentenaCoronavirus\n",
      "#cuarentenamexico\n",
      "#CulturaEnCasa\n",
      "#Cuarentena\n",
      "#AburridoEnCasa\n",
      "#Covid19\n",
      "#sabadodecuarentena\n",
      "#EnCuarentena\n",
      "#QuedateEnLaCasa\n",
      "#CuidarnosEsTareaDeTodos\n",
      "#QuédateEnCasaUnMesMas\n",
      "#quedateencasa\n",
      "#Cuidate\n",
      "#CuidaALosTuyos\n",
      "#CuidaALosDemas\n",
      "#SeFuerteMexico\n",
      "#AislamientoSocial\n",
      "#Enfermera\n",
      "#covidmexico\n",
      "#QuedateEnCasa\n",
      "#COVID19\n",
      "#SusanaDistancia\n",
      "#YaBastaDeFakeNews\n",
      "#MeQuedoEnHome\n",
      "#QuePorMiNoQuede\n",
      "#EnCuarentena\n",
      "#Covid_19\n",
      "#COVID-19\n",
      "#QuedarseEnCasa\n",
      "#YoMeQuedoEnCASA\n",
      "#quédate\n",
      "#SaltilloQuédateEnCasa\n",
      "#cuerentenadeilustración\n",
      "#BastaDeFakeNews\n",
      "#CuandoEstoSeAcabe\n",
      "#ConferenciaCovid19\n",
      "#StayAtHome\n",
      "#mequedoencasa\n",
      "#QuedateEnTuCasaCarajo\n",
      "#SiTeSalesTeMueres\n",
      "#yolecreoagattel\n",
      "coronavac\n",
      "covaxin\n",
      "johnson & johnson\n",
      "johnson&johnson\n",
      "sputnik v\n",
      "sputnik-v\n",
      "sputnikv\n",
      "sinopharm\n",
      "sinovac\n",
      "PCR\n",
      "HLGatell\n",
      "nosalirdecasa\n",
      "no salir de casa\n",
      "antígeno\n",
      "antigeno\n",
      "casos? positivos?\n",
      "clases presenciales\n",
      "clasespresenciales\n"
     ]
    }
   ],
   "source": [
    "class Per ():\n",
    "    \n",
    "    \n",
    "    def __init__(self,db) :\n",
    "        self.db = db\n",
    "        karl = \"jijiji\"\n",
    "        seo = karl + \"hioka\"\n",
    "        self.peop = seo\n",
    "        \n",
    "        \n",
    "    \n",
    "    def printDicc (self):\n",
    "        print(self.peop)\n",
    "        sql = \"SELECT diccionario_tema FROM temas  WHERE tema LIKE 'COVID';\"\n",
    "        ndic = self.db.selectOne(sql)\n",
    "        print(ndic)\n",
    "        \n",
    "        \n",
    "        \n",
    "Per(db).printDicc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión Exitosa\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "class DataBase:\n",
    "    def __init__(self):\n",
    "        self.connection = pymysql.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            password=\"Password.03\",\n",
    "            db=\"miopers1\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.cursor = self.connection.cursor()\n",
    "        print(\"Conexión Exitosa\")\n",
    "    \n",
    "    def selectOne (self, sql):\n",
    "        try:\n",
    "            self.cursor.execute(sql)\n",
    "            edo = self.cursor.fetchone()\n",
    "            return edo[0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "        \n",
    "    def selectAll (self, sql):\n",
    "        try:\n",
    "            self.cursor.execute(sql)\n",
    "            edo = self.cursor.fetchall()\n",
    "            return edo\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "        \n",
    "    def update (self, sql):\n",
    "        try:\n",
    "            self.cursor.execute(sql)\n",
    "            self.connection.commit()\n",
    "            print(\"operación exitosa\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    def nTema (self, tema, actualizar, diccionario,):\n",
    "        sql = \"INSERT INTO temas (tema, actualizar, diccionario_tema) values ('{}', {}, '{}');\".format(tema, actualizar, diccionario)\n",
    "        self.update(sql)\n",
    "    \n",
    "    def nAnalisisTema (self, idTema, descripcion, path, dicc, actualizar):\n",
    "        if dicc:\n",
    "            sql = \"INSERT INTO analisis_por_temas (id_tema, descripcion_analisis, path_analisis, diccionario_analisis, actualizar) values ({},'{}', '{}', '{}',{});\".format(idTema, descripcion, path, dicc, actualizar)\n",
    "        else:\n",
    "            sql = \"INSERT INTO analisis_por_temas (id_tema, descripcion_analisis, path_analisis, diccionario_analisis, actualizar) values ({},'{}', '{}',NULL,{});\".format(idTema, descripcion, path, actualizar)   \n",
    "        self.update(sql)\n",
    "        \n",
    "    def getCurrentProceso (self):\n",
    "        sql = \"SELECT max(id_proceso) from proceso;\"\n",
    "        return self.selectOne(sql)\n",
    "    \n",
    "    def getIdAnalisis (self, analisis):\n",
    "        sql = \"SELECT id_analisis FROM analisis_por_temas WHERE descripcion_analisis LIKE '{}';\".format(analisis)\n",
    "        return self.selectOne(sql)\n",
    "    \n",
    "    def getAnalisisDicc (self, analisis):\n",
    "        sql = \"SELECT diccionario_analisis FROM analisis_por_temas WHERE descripcion_analisis LIKE '{}';\".format(analisis)\n",
    "        return self.selectOne(sql)\n",
    "    \n",
    "    def getAllAnalisisTopic (self, analisis):\n",
    "        #obtiene todas las descripciones con sus ids a partir del nombre del analisis\n",
    "        sql = \"SELECT ad.id_detalle, ad.descripcion FROM analisis_detalle ad, analisis_por_temas apt where ad.id_analisis = apt.id_analisis and apt.descripcion_analisis like '{}';\".format(analisis)\n",
    "        return self.selectAll(sql)\n",
    "    \n",
    "    def getIDLoc (self,lugar):\n",
    "        sql = \"SELECT id_localidad FROM localidad WHERE nombre LIKE '{}';\".format(lugar)\n",
    "        return self.selectOne(sql)\n",
    "    \n",
    "    def getAllLocChild(self,id_locPadre) -> dict():\n",
    "        sql = \"SELECT nombre, id_localidad FROM  localidad WHERE id_loc_padre = {};\".format(id_locPadre)\n",
    "        mun = dict()\n",
    "        for t in db.selectAll(sql):\n",
    "            mun[t[0]] = t[1]\n",
    "        \n",
    "        return mun\n",
    "            \n",
    "    def close (self):\n",
    "        self.connection.close()\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "db = DataBase()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear tema nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tema = \"COVID\"\n",
    "actualizar = 1 # 1 para actualizar y 0 para no actualizar\n",
    "#En la variable data cargamos el diccionario a añadir en el tema\n",
    "filter_path = \"/home/adrian/Miopers/master/data/diccionarios/filtro_covid.txt\"\n",
    "with open(filter_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "db.nTema(tema, actualizar, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear analisis por tema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#timeline\n",
    "idTema = 1\n",
    "descripcion = \"timeline\"\n",
    "path = \"/home/miopers/Miopers/home/src/Linea_de_tiempo/timeline.py\"\n",
    "dicc =  \"quedateencasa,coronavirus, covid19,@HLGatell,@lopezobrador_,SusanaDistancia\"\n",
    "actualizar = 1\n",
    "\n",
    "db.nAnalisisTema(idTema, descripcion, path, dicc, actualizar)\n",
    "\n",
    "#words\n",
    "idTema = 1\n",
    "descripcion = \"words\"\n",
    "path = \"/home/miopers/Miopers/home/src/Palabras_clave/words.py\"\n",
    "dicc = None\n",
    "actualizar = 1\n",
    "\n",
    "db.nAnalisisTema(idTema, descripcion, path, dicc, actualizar)\n",
    "\n",
    "#hashtags\n",
    "idTema = 1\n",
    "descripcion = \"hashtags\"\n",
    "path = \"/home/miopers/Miopers/home/src/Estados_hashtags/mexico_hashtags.py\"\n",
    "dicc = '{ \"Aguascalientes\":[\"aguascalientes\"], \"Baja California\":[\"mexicali\",\"tijuana\",\"baja california\",\"ensenada\"], \"Baja California Sur\":[\"la paz\",\"baja california sur\"], \"Campeche\":[\"campeche\"], \"Ciudad de México\":[\"cdmx\",\"distrito federal\",\"benito juarez\", \"cuajimalpa\",\"cuajimalpa de morelos\",\"alvaro obregon\",\"naucalpan\",\"naucalpan de juarez\", \"cuautitlan izcalli\",\"cuauhtemoc\",\"tlalpan\",\"coyoacan\", \"gustavo a. madero\",\"venustiano carranza\",\"miguel hidalgo\",\"xochimilco\", \"azcapotzalco\",\"iztacalco\",\"district federal\",\"distretto federale\",\"milpa alta\",\"tlahuac\"], \"Chihuahua\":[\"chihuahua\"], \"Chiapas\":[\"chiapas\",\"tuxtla gutierrez\"], \"Coahuila\":[\"coahuila\",\"saltillo\",\"coahuila de zaragoza\"], \"Colima\":[\"colima\"], \"Durango\":[\"durango\"], \"Guanajuato\":[\"guanajuato\",\"celaya\",\"apodaca\",\"irapuato\"], \"Guerrero\":[\"guerrero\",\"chilpancingo\",\"acapulco\",\"acapulco de juarez\", \"chilpancingo de los bravo\"], \"Hidalgo\":[\"hidalgo\",\"pachuca\"], \"Jalisco\":[\"jalisco\",\"guadalajara\",\"puerto vallarta\",\"zapopan\",\"tlaquepaque\"], \"Estado de México\":[\"estado de mexico\",\"toluca\",\"tlalnepantla\",\"tlalnepantla de baz\",\"nezahualcoyotl\" ,\"atizapan de zaragoza\",\"ixtapaluca\",\"iztapalapa\",\"ecatepec de morelos\",\"huixquilucan\",\"tultitlan\"], \"Michoacán\":[\"michoacan\",\"morelia\",\"michoacan de ocampo\"], \"Morelos\":[\"morelos\",\"cuernavaca\"], \"Nayarit\":[\"nayarit\",\"tepic\"], \"Nuevo León\":[\"nuevo leon\",\"monterrey\"], \"Oaxaca\":[\"oaxaca\",\"villa diaz ordaz\",\"oaxaca de juarez\"], \"Puebla\":[\"puebla\",\"san pedro cholula\",\"san andres cholula\"], \"Querétaro\":[\"queretaro\",\"queretaro arteaga\"], \"Quintana Roo\":[\"quintana roo\",\"chetumal\"], \"San Luis Potosí\":[\"san luis potosi\"], \"Sinaloa\":[\"sinaloa\",\"culiacan\"], \"Sonora\":[\"sonora\",\"hermosillo\"], \"Tabasco\":[\"tabasco\",\"villahermosa\"], \"Tamaulipas\":[\"tamaulipas\",\"ciudad victoria\",\"tampico\",\"reynosa\"], \"Tlaxcala\":[\"tlaxcala\"], \"Veracruz\":[\"veracruz\",\"xalapa\",\"boca del rio\",\"cordoba\",\"veracruz de ignacio de la llave\",\"panuco\", \"poza rica de hidalgo\"], \"Yucatán\":[\"yucatan\",\"merida\"], \"Zacatecas\":[\"zacatecas\"]}'\n",
    "actualizar = 1\n",
    "\n",
    "db.nAnalisisTema(idTema, descripcion, path, dicc, actualizar)\n",
    "\n",
    "#sintomas\n",
    "idTema = 1\n",
    "descripcion = \"sintomas\"\n",
    "path = \"/home/miopers/Miopers/sintomas/src/analisis_sintomas.py\"\n",
    "dicc = '{\"mentales\": [\"salud mental\", \"depresion\", \"depresivo\", \"cansado\", \"cansancio\", \"fatigado\", \"fatiga\", \"cansada\", \"fatigada\", \"insomnio\", \"falta de sueño\", \"enojado\", \"enojada\", \"irritable\", \"irritante\", \"irritado\", \"irritada\", \"poca hambre\", \"sin hambre\", \"sin apetito\", \"poco apetito\", \"frustacion\", \"frustado\", \"frustada\", \"acomplejado\", \"acomplejada\", \"me cuesta concentrarme\", \"sin concentracion\", \"concentrado\", \"concentrada\", \"me siento inutil\", \"inservible\", \"ansiedad\", \"ansioso\", \"ansiosa\", \"crisis de ansiedad\", \"dolor de espalda\", \"temblores\", \"me late muy rapido el corazon\", \"tension\", \"nervioso\", \"nerviosa\", \"me agito rapido\", \"me preocupo demasiado\", \"me da miedo\", \"miedo\", \"miedosa\", \"miedoso\", \"miedito\", \"miedillo\", \"miedita\", \"ansiosos\", \"taquicardia\", \"sueño\", \"somnolencia\", \"dormir\", \"despertar\", \"cama\", \"cansancio\", \"fatica\", \"fatigo\", \"fatigado\", \"fatigada\", \"fatigados\", \"fatigadas\", \"irritable\", \"insoportable\", \"soportable\", \"insoportables\", \"temblor\", \"temblores\", \"pasa algo\", \"algo está pasando\", \"sentimiento\", \"llorar\", \"lloré\", \"lloró\", \"lloramos\", \"lloro\", \"llore\", \"lloremos\", \"sensibilidad\", \"hipersensible\", \"hipersensibilidad\", \"sentimental\", \"chipioso\", \"chipil\", \"aberración\", \"aberracion\", \"aberrante\", \"aberrantes\", \"ablutomanía\", \"abatimiento\", \"abreacción\", \"cerebral\", \"cerebro\", \"cerebelo\", \"abstemio\", \"abstinencia\", \"síndrome\", \"complejo\", \"problema\", \"abulia\", \"hipobulia\", \"aburrimiento\", \"aburrido\", \"aburrición\", \"aburrision\", \"aburricion\", \"aburrirse\", \"aburriendose\", \"aburriendonos\", \"sustancias\", \"abuso\", \"abusar\", \"abusó\", \"abusaron\", \"animo\", \"ánimo\", \"estado de ánimo\", \"bajo\", \"dificultad para dormir\", \"dificultad para conciliar\", \"conciliar sueño\", \"exceso de sueño\", \"no me puedo parar de la cama\", \"hambre\", \"hambriento\", \"apetito\", \"perdida de peso\", \"pérdida de peso\", \"pérdida\", \"peso\", \"gordo\", \"engordar\", \"concentrarse\", \"difícil concentrarser\", \"concentrarme\", \"concentración\", \"lento\", \"letargo\", \"letargado\", \"desesperanza\", \"perder la esperanza\", \"perdí esperanza\", \"esperanza\", \"abandono\", \"abandonar\", \"placer\", \"pérdida de placer\", \"me hacía feliz\", \"me gustaba\", \"nerviosismo\", \"nervioso\", \"agitado\", \"agitación\", \"tenso\", \"tensado\", \"tensión\", \"peligro\", \"inminente\", \"pánico\", \"paniqueado\", \"paniqueada\", \"catástrofe\", \"respiración acelerada\", \"hiperventilar\", \"sudoración\", \"sudor\", \"sudé\", \"sudo\", \"temblores\", \"debil\", \"debilidad\", \"problemas para concentrarse\", \"concentrarse\", \"concentrar\", \"conciliar\", \"problemas estomacáles\", \"preocupaciones\", \"evitar situaciones\"], \"sintomas\": [\"dolor de cabeza\", \"cuerpo cortado\", \"tos\", \"fiebre\", \"dolor de espalda\", \"dolor de garganta\", \"cansancio\", \"diarrea\", \"cefalea\", \"fiebre\", \"temperatura\", \"calentura\", \"jaqueca\", \"jaqueka\", \"estornudo\", \"hiperventilar\", \"falta de aire\", \"sin fuerza\", \"debil\", \"debilidad\", \"irritacion\", \"dolor de cuerpo\", \"cuerpo cortado\", \"fragil\", \"fragilidad\", \"frágil\", \"fiebre\", \"fiebroso\", \"temperatura\", \"tos\", \"tos seca\", \"tos rasposa\", \"garganta\", \"estómago\", \"panaa\", \"estomago\", \"panza\", \"pansa\", \"cansancio\", \"faatiga\", \"fatigado\", \"fatigada\", \"adormecimiento\", \"adormecido\", \"olfato\", \"conjuntivitis\", \"ojos\", \"garganta\", \"diarrea\", \"pérdida de gusto\", \"erupciones\", \"cutáneo\", \"cutánea\", \"cambios de color\", \"dedos\", \"pies\", \"manos\", \"garganta\", \"gañote\", \"saliva\", \"pasar saliva\", \"baba\", \"intenso\", \"dolor\", \"dolores\", \"pérdida de\", \"perdida de\", \"garganta reseca\", \"ardor de ojos\", \"ardor\", \"hipertensión\", \"hipertension\", \"arterial\", \"arteria\", \"arterias\", \"cardiaco\", \"pulmonar\", \"corazón\", \"corazon\", \"corason\", \"pulmones\", \"pulmón\", \"pulmon\", \"diabetes\", \"diabetis\", \"cáncer\", \"cancer\", \"respieren\", \"respiro\", \"respirar\", \"opresión\", \"pecho\", \"dolor en el pecho\", \"dificultad para\", \"me dificulta\"]}'\n",
    "actualizar = 1\n",
    "\n",
    "db.nAnalisisTema(idTema, descripcion, path, dicc, actualizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idTema = 1\n",
    "descripcion = \"words\"\n",
    "path = \"/home/miopers/Miopers/home/src/Palabras_clave/words.py\"\n",
    "dicc =  \"quedateencasa,coronavirus, covid19,@HLGatell,@lopezobrador_,SusanaDistancia\"\n",
    "actualizar = 1\n",
    "\n",
    "db.nAnalisisTema(idTema, descripcion, path, dicc, actualizar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga de tweets, creación de proceso, almacenamiento de los ID'S de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "#Establecimiento de Parámetros\n",
    "MONGO_DB = 'twitterdb'\n",
    "MONGO_HOST = '132.247.22.53'\n",
    "MONGO_USER = 'ConsultaTwitter'\n",
    "MONGO_PASS = '$Con$ulT@C0V1D'\n",
    "MONGO_MECHANISM = 'SCRAM-SHA-25'\n",
    "\n",
    "#Línea de Conexión Productiva\n",
    "uri = f'mongodb://{quote_plus(MONGO_USER)}:{quote_plus(MONGO_PASS)}@{MONGO_HOST}/{MONGO_DB}'\n",
    "\n",
    "client3 = MongoClient(uri)\n",
    "db3 = client3.twitterdb\n",
    "collection = db3.tweetsMexico\n",
    "\n",
    "def fechas_milisegundos(fecha_inicial, fecha_final):\n",
    "    \"\"\"recibe fechas en formato YYYY-mm-dd y las regresa en milisegundos, la fecha final la regresa a las 0:00 horas i.e. es al comienzo del dia\"\"\"\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    fechaInicial = datetime.strptime(fecha_inicial, formato_fecha)\n",
    "    #milliseconds1 = int(round(fechaInicial.timestamp() * 1000))-18000000 #se restan los milisegundos a 5 horas para coincidir con la zona horaria que traen los tweets \n",
    "    milliseconds1 = int(round(fechaInicial.timestamp() * 1000))#- 21600000 #se restan los milisegundos a 6 horas para coincidir con la zona horaria que traen los tweets \n",
    "    \n",
    "    fechaFinal = datetime.strptime(fecha_final, formato_fecha)\n",
    "    #milliseconds2 = int(round(fechaFinal.timestamp() * 1000))-18000000\n",
    "    milliseconds2 = int(round(fechaFinal.timestamp() * 1000))#-21600000\n",
    "    return milliseconds1,milliseconds2  \n",
    "\n",
    "def dataImport(diccWords) -> List[str]:\n",
    "    \"\"\"lee una lista de palabras, cada renglón es un elemento de la lista resultante\"\"\"\n",
    "    tokens = []\n",
    "    for word in diccWords.split('\\n'):\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def crearExpresion(terminos):\n",
    "    \"\"\"recibe una lista de terminos, los concatena con | para crear  una regex\"\"\"\n",
    "    def concatenarOR(elemento1, elemento2):\n",
    "        \"\"\"Recibe dos strings, las concatena separándolas con símbolo |\"\"\"\n",
    "        return elemento1+\"|\"+elemento2\n",
    "    return reduce(concatenarOR,terminos)\n",
    "\n",
    "#para fechas especificas\n",
    "def consulta(fecha_inicial, fecha_final):\n",
    "    \"\"\"obtiene todos los tweets relacionados al coronavirus dentro de las fechas especificadas. Regresa una lista de documentos (diccionarios).\"\"\"\n",
    "    sql = \"SELECT diccionario_tema FROM temas  WHERE tema LIKE 'COVID';\"\n",
    "    ndic = db.selectOne(sql) #para modificar cuando se convierta a class\n",
    "    expresion = crearExpresion(dataImport(ndic))\n",
    "    regex = re.compile(expresion, re.I)\n",
    "    tiempo1, tiempo2 = fechas_milisegundos(fecha_inicial, fecha_final)\n",
    "    pipeline = [\n",
    "        {\n",
    "            '$match': {\n",
    "                '$and': [\n",
    "                    {\n",
    "                        'timestamp_ms': {\n",
    "                            '$gte': str(tiempo1)\n",
    "                        }\n",
    "                    }, \n",
    "                    {\n",
    "                        'timestamp_ms': {\n",
    "                            '$lt': str(tiempo2)\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            '$match': {\n",
    "                '$or': [\n",
    "                    {\n",
    "                        'truncated': False,\n",
    "                        'text': {\n",
    "                            '$regex': regex\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'truncated': True,\n",
    "                        'extended_tweet.full_text': {\n",
    "                            '$regex': regex\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    cursor = collection.aggregate(pipeline)\n",
    "    return list(cursor)\n",
    "\n",
    "def consulta_sin_filtro(fecha_inicial, fecha_final):\n",
    "    \"\"\"obtiene todos los tweets contenidos en el servidor de Miopers en las fechas especificadas. Regresa una lista de documentos (diccionarios)\"\"\"\n",
    "    tiempo1, tiempo2 = fechas_milisegundos(fecha_inicial, fecha_final)\n",
    "    myquery = {'$expr':{'$and':[\n",
    "            { '$gte':[  { '$toLong': \"$timestamp_ms\"}, tiempo1 ]},\n",
    "            { '$lt':[  { '$toLong': \"$timestamp_ms\"}, tiempo2 ]}]}\n",
    "               }\n",
    "    #proyeccion = ['text'] #de cada documento solo descargamos los campos que nos interesan\n",
    "    #cursor = collection.find(filter=myquery, projection=proyeccion)\n",
    "    cursor = collection.find(filter=myquery)\n",
    "    tweets = list(cursor)\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def fechas_ayer_hoy_string():\n",
    "    \"\"\"Obtener la fecha de ayer y hoy en formato YYYY-mm-dd (con 0:00 horas)\"\"\"\n",
    "    hoy = datetime.now()\n",
    "    hoy_str = hoy.strftime(\"%Y-%m-%d\")\n",
    "    ayer = datetime.strptime(hoy_str, \"%Y-%m-%d\") - timedelta(days=1)\n",
    "    ayer_str = ayer.strftime(\"%Y-%m-%d\")\n",
    "    return ayer_str, hoy_str\n",
    "\n",
    "    \n",
    "def consulta_ayer():\n",
    "    \"\"\"Descarga los tweets del día anterior (ya filtrados)\"\"\"\n",
    "    #obtener fecha de ayer y hoy en formato dd-mm-YYYY\n",
    "    ayer_str, hoy_str = fechas_ayer_hoy_string()\n",
    "    \n",
    "    return consulta(ayer_str, hoy_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando tweets...\n",
      "finish\n",
      "INSERT INTO proceso (id_tema, conteo_tweet) VALUES (1, 218);\n",
      "operación exitosa\n",
      "El id de proceso es :  19\n",
      "INSERT INTO tweet (id_proceso, id_tweet) VALUES (19,'1615515399688257538'),(19,'1615516795107934209'),(19,'1615517133902954496'),(19,'1615517752244174848'),(19,'1615522062118055936'),(19,'1615524304384589824'),(19,'1615527133564002304'),(19,'1615527815285473281'),(19,'1615528277799763969'),(19,'1615531231193759745'),(19,'1615532272819519490'),(19,'1615533918362701824'),(19,'1615535751806226433'),(19,'1615536268766547968'),(19,'1615536615115431940'),(19,'1615536848637419520'),(19,'1615537955640340480'),(19,'1615538349204193281'),(19,'1615540616846532610'),(19,'1615541034758574080'),(19,'1615541042778116099'),(19,'1615542237236011008'),(19,'1615542416408301574'),(19,'1615542491322941442'),(19,'1615542942990757888'),(19,'1615543637823180800'),(19,'1615545589155958785'),(19,'1615547806730670082'),(19,'1615548171689631746'),(19,'1615551778820620288'),(19,'1615552262155247618'),(19,'1615552531207454721'),(19,'1615552826209636353'),(19,'1615552839895625728'),(19,'1615554369101107200'),(19,'1615554503801032704'),(19,'1615555129624633355'),(19,'1615555339679846400'),(19,'1615556529482256384'),(19,'1615558224647970821'),(19,'1615558260756471808'),(19,'1615558552621551617'),(19,'1615560743490928640'),(19,'1615563499228520456'),(19,'1615564408058511368'),(19,'1615565178036264964'),(19,'1615565460950351872'),(19,'1615566660533895168'),(19,'1615567021995102208'),(19,'1615568918046978048'),(19,'1615569089908310022'),(19,'1615570455661789184'),(19,'1615570659089973248'),(19,'1615572441463390208'),(19,'1615574005964242944'),(19,'1615574006974783489'),(19,'1615574506768322561'),(19,'1615574792282800128'),(19,'1615574865846890497'),(19,'1615579454801154049'),(19,'1615585116176228353'),(19,'1615588328866291712'),(19,'1615588566863646722'),(19,'1615589959540551680'),(19,'1615590799172927489'),(19,'1615592302470217728'),(19,'1615608248836542465'),(19,'1615609096241020928'),(19,'1615628505806704640'),(19,'1615632681156452352'),(19,'1615640245768273922'),(19,'1615641372098433025'),(19,'1615642345105510400'),(19,'1615677256004771840'),(19,'1615678747730116608'),(19,'1615679283447332864'),(19,'1615680494573600769'),(19,'1615680807527391236'),(19,'1615683614129152000'),(19,'1615693898730405888'),(19,'1615697332636647424'),(19,'1615697863165771776'),(19,'1615698875406290944'),(19,'1615699142910611462'),(19,'1615699466387951617'),(19,'1615699827240882178'),(19,'1615702517215817729'),(19,'1615707029825282050'),(19,'1615708725561409541'),(19,'1615708891144151040'),(19,'1615710793105817606'),(19,'1615711032428699648'),(19,'1615711699650101248'),(19,'1615713558020456449'),(19,'1615716214688432131'),(19,'1615716594931441664'),(19,'1615719065951080451'),(19,'1615720797204615173'),(19,'1615721811412488195'),(19,'1615721917205331971'),(19,'1615723240088903683'),(19,'1615724185875005442'),(19,'1615724216053301248'),(19,'1615724463965741058'),(19,'1615724826492116994'),(19,'1615725386570268672'),(19,'1615725838678503425'),(19,'1615725994509504520'),(19,'1615725995884953600'),(19,'1615726501936394241'),(19,'1615727196538273795'),(19,'1615727370945851393'),(19,'1615728583565082624'),(19,'1615731452041199618'),(19,'1615731970461552640'),(19,'1615732543390683138'),(19,'1615734290301521922'),(19,'1615735218324774912'),(19,'1615736066471772160'),(19,'1615736372756680704'),(19,'1615736445041262592'),(19,'1615736859983843334'),(19,'1615739360204259329'),(19,'1615740572332953600'),(19,'1615741312782962688'),(19,'1615742688644534272'),(19,'1615742950238978048'),(19,'1615743173544013824'),(19,'1615744139848716289'),(19,'1615745870376632325'),(19,'1615747012590174222'),(19,'1615748262258180096'),(19,'1615751237525463042'),(19,'1615751471680585731'),(19,'1615751592350818304'),(19,'1615751688400560128'),(19,'1615751896811311104'),(19,'1615753050534060032'),(19,'1615753267144687619'),(19,'1615755418403500032'),(19,'1615755877688176640'),(19,'1615755883325030402'),(19,'1615756319772020738'),(19,'1615760142569439234'),(19,'1615760346110889984'),(19,'1615761849105907718'),(19,'1615762730606002207'),(19,'1615763638521823247'),(19,'1615763724102696961'),(19,'1615763739097337857'),(19,'1615764010829217810'),(19,'1615764434793660459'),(19,'1615765678270578691'),(19,'1615765826615017473'),(19,'1615765990897221646'),(19,'1615766648396599302'),(19,'1615767237243060230'),(19,'1615768110405156864'),(19,'1615770737029128193'),(19,'1615773949719191552'),(19,'1615776328380485635'),(19,'1615776485931118593'),(19,'1615779478000791553'),(19,'1615779485340831745'),(19,'1615780836497833990'),(19,'1615784714333208592'),(19,'1615785504179621890'),(19,'1615785701152276513'),(19,'1615787428559867904'),(19,'1615787870584717328'),(19,'1615788478251286562'),(19,'1615789206265286656'),(19,'1615790605279596549'),(19,'1615790877296975872'),(19,'1615791497156169750'),(19,'1615791530819653661'),(19,'1615794300679294976'),(19,'1615795020560220160'),(19,'1615800515887087616'),(19,'1615800952623333378'),(19,'1615803648432345089'),(19,'1615805713044934669'),(19,'1615806278944841729'),(19,'1615807132321910787'),(19,'1615812815075348487'),(19,'1615813854168248320'),(19,'1615815302238937114'),(19,'1615815957632393218'),(19,'1615816417471000576'),(19,'1615818705165815808'),(19,'1615821269483786240'),(19,'1615821962256977920'),(19,'1615823487125393409'),(19,'1615827517390131200'),(19,'1615831506294906881'),(19,'1615833145240166411'),(19,'1615835025710415872'),(19,'1615838405753020435'),(19,'1615839909809364995'),(19,'1615839941505728515'),(19,'1615840885006077964'),(19,'1615841671844872192'),(19,'1615842808467787778'),(19,'1615846866750476294'),(19,'1615850433876201473'),(19,'1615851444636372992'),(19,'1615856390307995648'),(19,'1615857778039427074'),(19,'1615859350979104769'),(19,'1615862424883843072'),(19,'1615865627985469443'),(19,'1615868795062476802'),(19,'1615870948808482816'),(19,'1615871304640655360'),(19,'1615872325081075713'),(19,'1615873812553437189'),(19,'1615874907425972224'),(19,'1615876185526771712');\n",
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "#!/usr/bin/env bash\n",
    "DISPLAY=':0.0'\n",
    "\n",
    "import sys\n",
    "if '/home/adrian/Miopers/home/src/Estados_hashtags' not in sys.path: sys.path.append('/home/adrian/Miopers/home/src/Estados_hashtags')\n",
    "if '/home/adrian/Miopers/home/src/Linea_de_tiempo' not in sys.path: sys.path.append('/home/adrian/Miopers/home/src/Linea_de_tiempo')\n",
    "if '/home/adrian/Miopers/home/src/Palabras_clave' not in sys.path: sys.path.append('/home/adrian/Miopers/home/src/Palabras_clave')\n",
    "if '/home/adrian/Miopers/sintomas/src' not in sys.path: sys.path.append('/home/adrian/Miopers/sintomas/src')\n",
    "if '/home/adrian/Miopers/sentimientos/src' not in sys.path: sys.path.append('/home/adrian/Miopers/sentimientos/src')\n",
    "#import consultaMongo as mongo\n",
    "\n",
    "\n",
    "#descargar tweets (ya filtrados)\n",
    "print(\"Descargando tweets...\")\n",
    "#tweets = mongo.consulta_ayer()\n",
    "tweets = consulta_ayer()\n",
    "print(\"finish\")\n",
    "\n",
    "#crear regsitro de proceso en la base de datos\n",
    "id_tema = 1 #corresponde al tema: COVID\n",
    "\n",
    "#tanto el id_proceso y fecha_inicio se generan automáticamente al realizar el insert\n",
    "\n",
    "sql = \"INSERT INTO proceso (id_tema, conteo_tweet) VALUES ({}, {});\".format(id_tema, len(tweets))\n",
    "print(sql)\n",
    "db.update(sql)\n",
    "\n",
    "sql = \"SELECT max(id_proceso) from proceso;\"\n",
    "\n",
    "\n",
    "\n",
    "currentProcess = db.selectOne(sql)\n",
    "print(\"El id de proceso es : \", currentProcess)\n",
    "\n",
    "def getId (tweets):\n",
    "    return tweets['id']\n",
    "\n",
    "sql = \"INSERT INTO tweet (id_proceso, id_tweet) VALUES \"\n",
    "for  i in tweets:\n",
    "    comp = \"({},'{}'),\".format(currentProcess, getId(i))\n",
    "    sql = sql + comp\n",
    "    \n",
    "sql = sql[: -1]\n",
    "sql = sql + \";\"\n",
    "\n",
    "print(sql)\n",
    "db.update(sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar la celda anterior se tienen los tweets filtrados con el tema de COVID, ahora se procede a almacenar los id's de los tweets filtrados para insertarlos en el proceso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para crear las observaciones de timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO analisis_detalle (id_analisis, descripcion) VALUES (5,'quedateencasa'),(5,'coronavirus'),(5,'covid19'),(5,'@HLGatell'),(5,'@lopezobrador_'),(5,'SusanaDistancia');\n",
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "#'quedateencasa','coronavirus','covid19','@HLGatell','@lopezobrador_','SusanaDistancia'\n",
    "\n",
    "sql = \"SELECT diccionario_analisis from analisis_por_temas WHERE id_tema = (SELECT id_tema from temas WHERE tema like 'COVID');\"\n",
    "dicc_analisis = db.selectOne(sql)\n",
    "\n",
    "observaciones = dicc_analisis.split(\",\")\n",
    "\n",
    "sql = \"SELECT id_analisis FROM analisis_por_temas WHERE descripcion_analisis like 'timeline';\"\n",
    "id_analisis = db.selectOne(sql)\n",
    "\n",
    "sql = \"INSERT INTO analisis_detalle (id_analisis, descripcion) VALUES \"\n",
    "for  i in observaciones:\n",
    "    comp = \"({},'{}'),\".format(id_analisis, i)\n",
    "    sql = sql + comp\n",
    "    \n",
    "sql = sql[: -1]\n",
    "sql = sql + \";\"\n",
    "\n",
    "print(sql)\n",
    "db.update(sql)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf8\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import urllib\n",
    "#from IO_json import ordenar\n",
    "\n",
    "#mongo --host 132.247.22.53:27017 -u ConsultaTwitter --authenticationMechanism SCRAM-SHA-256 --authenticationDatabase twitterdb\n",
    "\n",
    "\n",
    "'''\n",
    "Palabras de contexto\n",
    "\n",
    "Se busca\n",
    "- Comportamiento del usuario\n",
    "- Estado de animo\n",
    "- Medidas del gobierno\n",
    "- Síntomas del usuario\n",
    "\n",
    "\n",
    "Para obtener contexto sacaré 2 palabras principales de 3 categorías:\n",
    "1.- Hashtags\n",
    "2.- Personas (Menciones)\n",
    "3.- Sintomas\n",
    "Con sus diferentes variaciones\n",
    "\n",
    "\n",
    "1.-\n",
    "- (covid19) covid covidmx\n",
    "- (coronavirus) virus\n",
    "- quedateencasa cuarentena encasa\n",
    "2.-\n",
    "- (Gatell)\n",
    "- (AMLO) andres manuel lopez obrador\n",
    "- SusanaDistancia\n",
    "3.-\n",
    "- (ansiedad)\n",
    "- (depresión)\n",
    "'''\n",
    "\n",
    "def getText(tweet):\n",
    "    \"\"\"\n",
    "    Summary: Función que permite extraer la información de los tweets descargados desde Monto para ser procesados.\n",
    "        Los textos de los tweets pueden ser descargados de forma total, truncada o el texto completo, dependiendo de\n",
    "        la calidad del tweet descargado. \n",
    "    Args:\n",
    "        tweet: Variable para identificar el texto de cada uno de los tweets descargados\n",
    "\n",
    "    Returns:\n",
    "        list: Se devuelve una lista con la información del texto de todos los tweets, estos pueden ser\n",
    "        de forma total o de forma parcial, dependiendo de la condición del tweet\n",
    "\n",
    "    \"\"\"\n",
    "    return tweet['text'] if not tweet['truncated'] else tweet['extended_tweet']['full_text']\n",
    "\n",
    "def getDate(tweet):\n",
    "    \"\"\"\n",
    "    Summary: Función para poder extraer de los tweets la fecha de cuando fueron emitidos cada uno de estos.\n",
    "    \n",
    "    Args:\n",
    "        tweet: Variable para identificar el texto de cada uno de los tweets descargados\n",
    "\n",
    "    Returns:\n",
    "        Se devuelve la fecha de publicación de cada uno de los tweets en formato datetime. \n",
    "\n",
    "    \"\"\"\n",
    "    timestamp = tweet['timestamp_ms'][:-3]\n",
    "    return datetime.fromtimestamp(int(timestamp)) + timedelta(hours=5)\n",
    "\n",
    "def ejecutarAnalisis(tweets):    \n",
    "    # Creamos una expresión regular para volver a extraer, aquellos tweets que tienen información relevante. \n",
    "    contexto = re.compile(r'(covid|(v|b)iru(s|z)|sars(-| )?cov|contingencia|sanitaria|sintoma|neumonia|quedateencasa|pandemia|encierro|cuarentena|encasa|cuandoestoseacabe|aislamientosocial|susana ?distancia)',re.I)\n",
    "\n",
    "    # Creamos un diccionario para poder guardar la información de los hashtags\n",
    "    timeline = dict()\n",
    "    # Información de las palabras claves y hastags relevantes para el estudio.  \n",
    "    tokens = ['quedateencasa','coronavirus', 'covid19' ,'@HLGatell','@lopezobrador_','SusanaDistancia']\n",
    "\n",
    "    # Inicialización del tiempo de ejecución del script\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Variable para poder contar el número de veces que aparece alguno de los topicos de interés en los tweets\n",
    "    total_contexto = 0\n",
    "\n",
    "    # Buscamos entre los tweets consultados de Mongo\n",
    "    for i,tweet in enumerate(tweets):\n",
    "        # Extraemos tweets, la fecha en que se emitieron y los almacenamos en variables\n",
    "        text = getText(tweet)\n",
    "        date = getDate(tweet).date()\n",
    "        data = timeline.get(str(date),{})\n",
    "        print('Fecha: ' + str(getDate(tweet)))\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "        # Vamos analizando cada uno de los tweets y contabilizamos el número de estos. \n",
    "        total_contexto = total_contexto + 1\n",
    "        # Buscamos cada uno de los tokens y si los hayamos en una lista guardamos un registro de ellos y aumentamos el contador\n",
    "        for token in tokens:\n",
    "            if token in text:\n",
    "                aux = data.get(token,0)\n",
    "                data[token] = aux + 1\n",
    "            else:\n",
    "                data[token] = data.get(token,0)\n",
    "            \n",
    "\n",
    "        timeline[str(date)] = data\n",
    "        \n",
    "    print(total_contexto)\n",
    "    print(\"a continuación imprimimos data: \\n\", data)\n",
    "    \n",
    "    #para ingresar los datos a la base de datos\n",
    "    \n",
    "    id_analisis = db.getIdAnalisis(\"timeline\")\n",
    "    id_proceso = db.getCurrentProceso()\n",
    "\n",
    "    sql = \"INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES \"\n",
    "\n",
    "    analisisTema = db.getAllAnalisisTopic(\"timeline\")\n",
    "\n",
    "    for  i in analisisTema:\n",
    "        if i[1] in data:\n",
    "            comp = \"({},{},{},{},NULL),\".format(i[0],id_analisis,id_proceso,data[i[1]])\n",
    "            sql = sql + comp\n",
    "    \n",
    "    sql = sql[: -1]\n",
    "    sql = sql + \";\"\n",
    "\n",
    "    print(sql)\n",
    "    \n",
    "    \n",
    "    db.update(sql)\n",
    "    \n",
    "    \n",
    "    #return data\n",
    "\n",
    "    # #descargamos un diccionario con la info historica\n",
    "    # with open(\"/home/adrian/Miopers/web/src/data/datatimeline_result.json\", \"r\") as json_file:\n",
    "    #     results = json.load(json_file)\n",
    "    \n",
    "    # # Creamos una nueva lista para almacenar la información de las fechas de los tweets\n",
    "    # data = list()\n",
    "    # for d in sorted(timeline.keys()):\n",
    "    #     # Vamos recorreiendo el diccionario creado con anterioridad y vemos la fecha de su creaación y los enumeramos\n",
    "    #     aux = timeline.get(d,{})\n",
    "    #     aux['day'] = d\n",
    "    #     data.append(aux)\n",
    "    #     results['data'].append(aux) #actualizar el diccionario con la info historica\n",
    "\n",
    "    # # Creamos un diccionario final para ir almacenando toda la información final.\n",
    "    # #results = dict()\n",
    "    \n",
    "    \n",
    "    # # Guardamos en el diccionario la información de cuando fue procesado el script.\n",
    "    # results['time'] = datetime.today().ctime()\n",
    "\n",
    "    # # Creamos un archivo .json donde guardamos la información de lo anteriormente procesado para su posterior uso. \n",
    "    # with open(\"/home/adrian/Miopers/web/src/data/timeline_result.json\", \"w\") as json_file:\n",
    "    #     json.dump(ordenar(results), json_file)\n",
    "    # #subir JSON al repositorio miopers\n",
    "    # #import IO_json\n",
    "    # #IO_json.upload_json(\"home/data/timeline_result.json\",results)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"home: linea de tiempo actualizada\\n--- %s segundos ---\" % (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha: 2023-01-18 00:04:20\n",
      "Fecha: 2023-01-18 00:09:53\n",
      "Fecha: 2023-01-18 00:11:14\n",
      "Fecha: 2023-01-18 00:13:41\n",
      "Fecha: 2023-01-18 00:30:49\n",
      "Fecha: 2023-01-18 00:39:43\n",
      "Fecha: 2023-01-18 00:50:58\n",
      "Fecha: 2023-01-18 00:53:40\n",
      "Fecha: 2023-01-18 00:55:31\n",
      "Fecha: 2023-01-18 01:07:15\n",
      "Fecha: 2023-01-18 01:11:23\n",
      "Fecha: 2023-01-18 01:17:55\n",
      "Fecha: 2023-01-18 01:25:13\n",
      "Fecha: 2023-01-18 01:27:16\n",
      "Fecha: 2023-01-18 01:28:38\n",
      "Fecha: 2023-01-18 01:29:34\n",
      "Fecha: 2023-01-18 01:33:58\n",
      "Fecha: 2023-01-18 01:35:32\n",
      "Fecha: 2023-01-18 01:44:33\n",
      "Fecha: 2023-01-18 01:46:12\n",
      "Fecha: 2023-01-18 01:46:14\n",
      "Fecha: 2023-01-18 01:50:59\n",
      "Fecha: 2023-01-18 01:51:42\n",
      "Fecha: 2023-01-18 01:51:59\n",
      "Fecha: 2023-01-18 01:53:47\n",
      "Fecha: 2023-01-18 01:56:33\n",
      "Fecha: 2023-01-18 02:04:18\n",
      "Fecha: 2023-01-18 02:13:07\n",
      "Fecha: 2023-01-18 02:14:34\n",
      "Fecha: 2023-01-18 02:28:54\n",
      "Fecha: 2023-01-18 02:30:49\n",
      "Fecha: 2023-01-18 02:31:53\n",
      "Fecha: 2023-01-18 02:33:03\n",
      "Fecha: 2023-01-18 02:33:07\n",
      "Fecha: 2023-01-18 02:39:11\n",
      "Fecha: 2023-01-18 02:39:43\n",
      "Fecha: 2023-01-18 02:42:13\n",
      "Fecha: 2023-01-18 02:43:03\n",
      "Fecha: 2023-01-18 02:47:46\n",
      "Fecha: 2023-01-18 02:54:31\n",
      "Fecha: 2023-01-18 02:54:39\n",
      "Fecha: 2023-01-18 02:55:49\n",
      "Fecha: 2023-01-18 03:04:31\n",
      "Fecha: 2023-01-18 03:15:28\n",
      "Fecha: 2023-01-18 03:19:05\n",
      "Fecha: 2023-01-18 03:22:08\n",
      "Fecha: 2023-01-18 03:23:16\n",
      "Fecha: 2023-01-18 03:28:02\n",
      "Fecha: 2023-01-18 03:29:28\n",
      "Fecha: 2023-01-18 03:37:00\n",
      "Fecha: 2023-01-18 03:37:41\n",
      "Fecha: 2023-01-18 03:43:07\n",
      "Fecha: 2023-01-18 03:43:55\n",
      "Fecha: 2023-01-18 03:51:00\n",
      "Fecha: 2023-01-18 03:57:13\n",
      "Fecha: 2023-01-18 03:57:13\n",
      "Fecha: 2023-01-18 03:59:12\n",
      "Fecha: 2023-01-18 04:00:21\n",
      "Fecha: 2023-01-18 04:00:38\n",
      "Fecha: 2023-01-18 04:18:52\n",
      "Fecha: 2023-01-18 04:41:22\n",
      "Fecha: 2023-01-18 04:54:08\n",
      "Fecha: 2023-01-18 04:55:05\n",
      "Fecha: 2023-01-18 05:00:37\n",
      "Fecha: 2023-01-18 05:03:57\n",
      "Fecha: 2023-01-18 05:09:55\n",
      "Fecha: 2023-01-18 06:13:17\n",
      "Fecha: 2023-01-18 06:16:39\n",
      "Fecha: 2023-01-18 07:33:47\n",
      "Fecha: 2023-01-18 07:50:22\n",
      "Fecha: 2023-01-18 08:20:26\n",
      "Fecha: 2023-01-18 08:24:54\n",
      "Fecha: 2023-01-18 08:28:46\n",
      "Fecha: 2023-01-18 10:47:30\n",
      "Fecha: 2023-01-18 10:53:25\n",
      "Fecha: 2023-01-18 10:55:33\n",
      "Fecha: 2023-01-18 11:00:22\n",
      "Fecha: 2023-01-18 11:01:37\n",
      "Fecha: 2023-01-18 11:12:46\n",
      "Fecha: 2023-01-18 11:53:38\n",
      "Fecha: 2023-01-18 12:07:16\n",
      "Fecha: 2023-01-18 12:09:23\n",
      "Fecha: 2023-01-18 12:13:24\n",
      "Fecha: 2023-01-18 12:14:28\n",
      "Fecha: 2023-01-18 12:15:45\n",
      "Fecha: 2023-01-18 12:17:11\n",
      "Fecha: 2023-01-18 12:27:53\n",
      "Fecha: 2023-01-18 12:45:48\n",
      "Fecha: 2023-01-18 12:52:33\n",
      "Fecha: 2023-01-18 12:53:12\n",
      "Fecha: 2023-01-18 13:00:46\n",
      "Fecha: 2023-01-18 13:01:43\n",
      "Fecha: 2023-01-18 13:04:22\n",
      "Fecha: 2023-01-18 13:11:45\n",
      "Fecha: 2023-01-18 13:22:18\n",
      "Fecha: 2023-01-18 13:23:49\n",
      "Fecha: 2023-01-18 13:33:38\n",
      "Fecha: 2023-01-18 13:40:31\n",
      "Fecha: 2023-01-18 13:44:33\n",
      "Fecha: 2023-01-18 13:44:58\n",
      "Fecha: 2023-01-18 13:50:13\n",
      "Fecha: 2023-01-18 13:53:59\n",
      "Fecha: 2023-01-18 13:54:06\n",
      "Fecha: 2023-01-18 13:55:05\n",
      "Fecha: 2023-01-18 13:56:32\n",
      "Fecha: 2023-01-18 13:58:45\n",
      "Fecha: 2023-01-18 14:00:33\n",
      "Fecha: 2023-01-18 14:01:10\n",
      "Fecha: 2023-01-18 14:01:10\n",
      "Fecha: 2023-01-18 14:03:11\n",
      "Fecha: 2023-01-18 14:05:57\n",
      "Fecha: 2023-01-18 14:06:38\n",
      "Fecha: 2023-01-18 14:11:27\n",
      "Fecha: 2023-01-18 14:22:51\n",
      "Fecha: 2023-01-18 14:24:55\n",
      "Fecha: 2023-01-18 14:27:11\n",
      "Fecha: 2023-01-18 14:34:08\n",
      "Fecha: 2023-01-18 14:37:49\n",
      "Fecha: 2023-01-18 14:41:11\n",
      "Fecha: 2023-01-18 14:42:24\n",
      "Fecha: 2023-01-18 14:42:42\n",
      "Fecha: 2023-01-18 14:44:21\n",
      "Fecha: 2023-01-18 14:54:17\n",
      "Fecha: 2023-01-18 14:59:06\n",
      "Fecha: 2023-01-18 15:02:02\n",
      "Fecha: 2023-01-18 15:07:30\n",
      "Fecha: 2023-01-18 15:08:33\n",
      "Fecha: 2023-01-18 15:09:26\n",
      "Fecha: 2023-01-18 15:13:16\n",
      "Fecha: 2023-01-18 15:20:09\n",
      "Fecha: 2023-01-18 15:24:41\n",
      "Fecha: 2023-01-18 15:29:39\n",
      "Fecha: 2023-01-18 15:41:28\n",
      "Fecha: 2023-01-18 15:42:24\n",
      "Fecha: 2023-01-18 15:42:53\n",
      "Fecha: 2023-01-18 15:43:16\n",
      "Fecha: 2023-01-18 15:44:06\n",
      "Fecha: 2023-01-18 15:48:41\n",
      "Fecha: 2023-01-18 15:49:32\n",
      "Fecha: 2023-01-18 15:58:05\n",
      "Fecha: 2023-01-18 15:59:55\n",
      "Fecha: 2023-01-18 15:59:56\n",
      "Fecha: 2023-01-18 16:01:40\n",
      "Fecha: 2023-01-18 16:16:52\n",
      "Fecha: 2023-01-18 16:17:40\n",
      "Fecha: 2023-01-18 16:23:38\n",
      "Fecha: 2023-01-18 16:27:09\n",
      "Fecha: 2023-01-18 16:30:45\n",
      "Fecha: 2023-01-18 16:31:05\n",
      "Fecha: 2023-01-18 16:31:09\n",
      "Fecha: 2023-01-18 16:32:14\n",
      "Fecha: 2023-01-18 16:33:55\n",
      "Fecha: 2023-01-18 16:38:51\n",
      "Fecha: 2023-01-18 16:39:27\n",
      "Fecha: 2023-01-18 16:40:06\n",
      "Fecha: 2023-01-18 16:42:43\n",
      "Fecha: 2023-01-18 16:45:03\n",
      "Fecha: 2023-01-18 16:48:31\n",
      "Fecha: 2023-01-18 16:58:57\n",
      "Fecha: 2023-01-18 17:11:43\n",
      "Fecha: 2023-01-18 17:21:11\n",
      "Fecha: 2023-01-18 17:21:48\n",
      "Fecha: 2023-01-18 17:33:41\n",
      "Fecha: 2023-01-18 17:33:43\n",
      "Fecha: 2023-01-18 17:39:05\n",
      "Fecha: 2023-01-18 17:54:30\n",
      "Fecha: 2023-01-18 17:57:38\n",
      "Fecha: 2023-01-18 17:58:25\n",
      "Fecha: 2023-01-18 18:05:17\n",
      "Fecha: 2023-01-18 18:07:02\n",
      "Fecha: 2023-01-18 18:09:27\n",
      "Fecha: 2023-01-18 18:12:21\n",
      "Fecha: 2023-01-18 18:17:54\n",
      "Fecha: 2023-01-18 18:18:59\n",
      "Fecha: 2023-01-18 18:21:27\n",
      "Fecha: 2023-01-18 18:21:35\n",
      "Fecha: 2023-01-18 18:32:35\n",
      "Fecha: 2023-01-18 18:35:27\n",
      "Fecha: 2023-01-18 18:57:17\n",
      "Fecha: 2023-01-18 18:59:01\n",
      "Fecha: 2023-01-18 19:09:44\n",
      "Fecha: 2023-01-18 19:17:56\n",
      "Fecha: 2023-01-18 19:20:11\n",
      "Fecha: 2023-01-18 19:23:35\n",
      "Fecha: 2023-01-18 19:46:10\n",
      "Fecha: 2023-01-18 19:50:17\n",
      "Fecha: 2023-01-18 19:56:03\n",
      "Fecha: 2023-01-18 19:58:39\n",
      "Fecha: 2023-01-18 20:00:28\n",
      "Fecha: 2023-01-18 20:09:34\n",
      "Fecha: 2023-01-18 20:19:45\n",
      "Fecha: 2023-01-18 20:22:30\n",
      "Fecha: 2023-01-18 20:28:34\n",
      "Fecha: 2023-01-18 20:44:35\n",
      "Fecha: 2023-01-18 21:00:26\n",
      "Fecha: 2023-01-18 21:06:57\n",
      "Fecha: 2023-01-18 21:14:25\n",
      "Fecha: 2023-01-18 21:27:51\n",
      "Fecha: 2023-01-18 21:33:50\n",
      "Fecha: 2023-01-18 21:33:57\n",
      "Fecha: 2023-01-18 21:37:42\n",
      "Fecha: 2023-01-18 21:40:50\n",
      "Fecha: 2023-01-18 21:45:21\n",
      "Fecha: 2023-01-18 22:01:28\n",
      "Fecha: 2023-01-18 22:15:39\n",
      "Fecha: 2023-01-18 22:19:40\n",
      "Fecha: 2023-01-18 22:39:19\n",
      "Fecha: 2023-01-18 22:44:50\n",
      "Fecha: 2023-01-18 22:51:05\n",
      "Fecha: 2023-01-18 23:03:18\n",
      "Fecha: 2023-01-18 23:16:01\n",
      "Fecha: 2023-01-18 23:28:36\n",
      "Fecha: 2023-01-18 23:37:10\n",
      "Fecha: 2023-01-18 23:38:35\n",
      "Fecha: 2023-01-18 23:42:38\n",
      "Fecha: 2023-01-18 23:48:33\n",
      "Fecha: 2023-01-18 23:52:54\n",
      "Fecha: 2023-01-18 23:57:58\n",
      "218\n",
      "a continuación imprimimos data: \n",
      " {'quedateencasa': 0, 'coronavirus': 0, 'covid19': 0, '@HLGatell': 35, '@lopezobrador_': 10, 'SusanaDistancia': 0}\n",
      "INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES (1,5,19,0,NULL),(2,5,19,0,NULL),(3,5,19,0,NULL),(4,5,19,35,NULL),(5,5,19,10,NULL),(6,5,19,0,NULL);\n",
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "ejecutarAnalisis(tweets) #timeline.py\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf8\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta, date\n",
    "import spacy\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "def getText(tweet):\n",
    "    \"\"\"\n",
    "    Summary: Función que permite extraer la información de los tweets descargados desde Monto para ser procesados.\n",
    "        Los textos de los tweets pueden ser descargados de forma total, truncada o el texto completo, dependiendo de\n",
    "        la calidad del tweet descargado. \n",
    "    Args:\n",
    "        tweet: Variable para identificar el texto de cada uno de los tweets descargados\n",
    "\n",
    "    Returns:\n",
    "        list: Se devuelve una lista con la información del texto de todos los tweets, estos pueden ser\n",
    "        de forma total o de forma parcial, dependiendo de la condición del tweet\n",
    "\n",
    "    \"\"\"\n",
    "    return tweet['text'] if not tweet['truncated'] else tweet['extended_tweet']['full_text']\n",
    "\n",
    "def getDate(tweet):\n",
    "    \"\"\"\n",
    "    Summary: Función para poder extraer de los tweets la fecha de cuando fueron emitidos cada uno de estos.\n",
    "    \n",
    "    Args:\n",
    "        tweet: Variable para identificar el texto de cada uno de los tweets descargados\n",
    "\n",
    "    Returns:\n",
    "        Se devuelve la fecha de publicación de cada uno de los tweets en formato datetime. \n",
    "\n",
    "    \"\"\"\n",
    "    timestamp = tweet['timestamp_ms'][:-3]\n",
    "    return datetime.fromtimestamp(int(timestamp)) + timedelta(hours=5)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Summary: Procesamos la información y la limpiamos utilizando NLP donde removemos acentos, y extraemos\n",
    "        las palabras principales que nos interesan en el re.compile\n",
    "\n",
    "    Args: \n",
    "        text: El texto del tweet que estamos analizando \n",
    "\n",
    "    Returns:\n",
    "        Devuelve el texto pricesado sin acentos y con las palabras de interes extraidas de igual forma\n",
    "        en formato de texto\n",
    "    \"\"\"\n",
    "    text = re.sub(r'http\\S+',' ',text)\n",
    "    text = re.sub(r'#(\\w+)',r'h2sht2g@\\1',text)\n",
    "    text = text.replace('á','a')\n",
    "    text = text.replace('é','e')\n",
    "    text = text.replace('í','i')\n",
    "    text = text.replace('ó','o')\n",
    "    text = text.replace('ú','u')\n",
    "    text = text.replace(',',' ')\n",
    "    text = text.replace('\\n',' ')\n",
    "    return text\n",
    "#recibir tweets y data_path\n",
    "\n",
    "def ejecutarAnalisis(tweets):\n",
    "    '''\n",
    "    Creamos 3 diccionarios diferentes para almacenar información relativa a los tweets que se generan y \n",
    "    los hashtags a los que hacen mencion. \n",
    "    '''\n",
    "    words = dict()\n",
    "    hashtags = dict()\n",
    "    mentions = dict()\n",
    "\n",
    "    # Inicializamos temporaizador para contar el tiempo de ejecución del script\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Inicializamos con spacy el procesamiento de información reuniendo los hashtags que nos intereaan por medio de NLP\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "\n",
    "    # Corremos un loop donde se consultan los tweets del momento. \n",
    "    for i,tweet in enumerate(tweets):\n",
    "        # Extraemos las fechas y textos de los tweets consultados\n",
    "        text = preprocess(getText(tweet).lower()).strip()\n",
    "        date = getDate(tweet)\n",
    "        print(i,date)\n",
    "\n",
    "        # Analizamos por medio del avariable nlp los tweets buscando la información de la lista de hashtags que nos interesan. \n",
    "        sentence = nlp(text)\n",
    "        # Una vez extraidos los tweets con hashtags objetivos procedemos a tokenizar y clasificar los hashtags\n",
    "        for token in sentence:\n",
    "            if token.text.startswith('h2sht2g@'):\n",
    "                aux = hashtags.get(token.text.replace('h2sht2g@','#'),0)\n",
    "                hashtags[token.text.replace('h2sht2g@','#')] = aux + 1\n",
    "            elif token.text.startswith('@'):\n",
    "                aux = mentions.get(token.text,0)\n",
    "                mentions[token.text] = aux + 1\n",
    "            else:\n",
    "                # En caso contrario tokenizar los tweets extrayendo los 'ADJ' y 'NOUN'\n",
    "                if 'ADJ' in token.pos_ or 'NOUN' in token.pos_:\n",
    "                    aux = words.get(token.text,0)\n",
    "                    words[token.text] = aux + 1\n",
    "\n",
    "\n",
    "    # Se crean dos listas en especifico para poder guardar las fechas de los tweets\n",
    "    data = list() # Sera una lista de diccionarios.\n",
    "    aux = dict()\n",
    "    dataDict = dict()\n",
    "\n",
    "    # Se crea un loop para poder contar la cantidad de twets ('t') y words ('w') que se lograron contabilizar.\n",
    "    for i, w in enumerate(sorted(hashtags,key=hashtags.get,reverse=True)):\n",
    "        #aux['t'+str(i+1)] = str(round(hashtags[w]/1000,2)) + 'k'\n",
    "        aux['t'+str(i+1)] = str(hashtags[w])\n",
    "        aux['w'+str(i+1)] = w\n",
    "        #print(w, hashtags[w])\n",
    "        if hashtags[w] > 1:\n",
    "            dataDict[w] = hashtags[w] #para ingresar datos a base de datos \n",
    "        # Si el son muy pocos ni siquiera correr el proceso. \n",
    "        #if not i < 2:\n",
    "         #   break\n",
    "\n",
    "    # Creamos gurdamos todos los tweets procesados anteriormente en la lista data \n",
    "    data.append(aux)\n",
    "    aux = dict()\n",
    "\n",
    "    # Creamos un loop donde se repite el mismo procedimiento pero esta vez para los tweets en los que fueron mencionados los usuarios de interés\n",
    "    for i, w in enumerate(sorted(mentions,key=mentions.get,reverse=True)):\n",
    "        aux['t'+str(i+1)] = str(mentions[w])\n",
    "        aux['w'+str(i+1)] = w\n",
    "        if mentions[w] > 1:\n",
    "            dataDict[w] = mentions[w] #para ingresar datos a base de datos \n",
    "        #if not i < 2:\n",
    "         #   break\n",
    "    # Se almacenan las menciones en la misma lista de datos. \n",
    "    data.append(aux)\n",
    "    aux = dict()\n",
    "\n",
    "    # Se repite el mismo procedimiento una 3ra vez pero ahora almacenamos las palabras de cada uno de los tweets y las guardamos en una lista. \n",
    "    for i, w in enumerate(sorted(words,key=words.get,reverse=True)):\n",
    "        aux['t'+str(i+1)] = str(words[w])\n",
    "        aux['w'+str(i+1)] = w\n",
    "        if words[w] > 1:\n",
    "            dataDict[w] = words[w] #para ingresar datos a base de datos \n",
    "        #if not i < 2:\n",
    "         #   break\n",
    "    data.append(aux)\n",
    "\n",
    "    # Creamos un diccionario para la información de los hashtags, mensiones y words y el momento en que fueron procesados \n",
    "    # y lo guardamos en un .json para su posterior uso. \n",
    "    \n",
    "    \n",
    "    arr = list(dataDict.keys())\n",
    "\n",
    "    wordsDB = db.getAllAnalisisTopic(\"words\")\n",
    "    listaPal = []\n",
    "\n",
    "    for i in wordsDB:\n",
    "        listaPal.append(i[1])\n",
    "\n",
    "    id_analisis = db.getIdAnalisis(\"words\")\n",
    "    #print(id_analisis)\n",
    "    cent = False\n",
    "\n",
    "\n",
    "    sql = \"INSERT INTO analisis_detalle (id_analisis, descripcion) VALUES \"\n",
    "    for  i in arr:\n",
    "        if i not in listaPal:\n",
    "            cent =True\n",
    "            comp = \"({},'{}'),\".format(id_analisis, i)\n",
    "            sql = sql + comp\n",
    "        \n",
    "    sql = sql[: -1]\n",
    "    sql = sql + \";\"\n",
    "\n",
    "    #print(sql)\n",
    "    if cent:\n",
    "        db.update(sql)\n",
    "\n",
    "    #para ingresar los datos a la base de datos\n",
    "        \n",
    "    id_analisis = db.getIdAnalisis(\"words\")\n",
    "    id_proceso = db.getCurrentProceso()\n",
    "    wordsDB = db.getAllAnalisisTopic(\"words\")\n",
    "    sql = \"INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES \"\n",
    "\n",
    "\n",
    "\n",
    "    for  i in wordsDB:\n",
    "        if i[1] in arr:\n",
    "            comp = \"({},{},{},{},NULL),\".format(i[0],id_analisis,id_proceso,dataDict[i[1]])\n",
    "            sql = sql + comp\n",
    "        \n",
    "    sql = sql[: -1]\n",
    "    sql = sql + \";\"\n",
    "\n",
    "    \n",
    "    #print(sql)\n",
    "        \n",
    "        \n",
    "    db.update(sql)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    results = dict()\n",
    "    results['time'] = datetime.today().ctime()\n",
    "    results['data'] = data\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataDict\n",
    "\n",
    "    # De la información procesada se crea un archivo .json con las palabras más frecuentes de los tweets\n",
    "    # with open('/home/adrian/Miopers/web/src/data/words_results.json', 'w') as f:\n",
    "    #     json.dump(results, f)\n",
    "    #actualizar JSON en el repo\tMiopers\n",
    "    #import IO_json\n",
    "    #IO_json.upload_json(\"home/data/words_results.json\",results)\n",
    "    \n",
    "    print(\"Palabras del dia actualizadas\\n--- %s segundos ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2023-01-18 00:04:20\n",
      "1 2023-01-18 00:09:53\n",
      "2 2023-01-18 00:11:14\n",
      "3 2023-01-18 00:13:41\n",
      "4 2023-01-18 00:30:49\n",
      "5 2023-01-18 00:39:43\n",
      "6 2023-01-18 00:50:58\n",
      "7 2023-01-18 00:53:40\n",
      "8 2023-01-18 00:55:31\n",
      "9 2023-01-18 01:07:15\n",
      "10 2023-01-18 01:11:23\n",
      "11 2023-01-18 01:17:55\n",
      "12 2023-01-18 01:25:13\n",
      "13 2023-01-18 01:27:16\n",
      "14 2023-01-18 01:28:38\n",
      "15 2023-01-18 01:29:34\n",
      "16 2023-01-18 01:33:58\n",
      "17 2023-01-18 01:35:32\n",
      "18 2023-01-18 01:44:33\n",
      "19 2023-01-18 01:46:12\n",
      "20 2023-01-18 01:46:14\n",
      "21 2023-01-18 01:50:59\n",
      "22 2023-01-18 01:51:42\n",
      "23 2023-01-18 01:51:59\n",
      "24 2023-01-18 01:53:47\n",
      "25 2023-01-18 01:56:33\n",
      "26 2023-01-18 02:04:18\n",
      "27 2023-01-18 02:13:07\n",
      "28 2023-01-18 02:14:34\n",
      "29 2023-01-18 02:28:54\n",
      "30 2023-01-18 02:30:49\n",
      "31 2023-01-18 02:31:53\n",
      "32 2023-01-18 02:33:03\n",
      "33 2023-01-18 02:33:07\n",
      "34 2023-01-18 02:39:11\n",
      "35 2023-01-18 02:39:43\n",
      "36 2023-01-18 02:42:13\n",
      "37 2023-01-18 02:43:03\n",
      "38 2023-01-18 02:47:46\n",
      "39 2023-01-18 02:54:31\n",
      "40 2023-01-18 02:54:39\n",
      "41 2023-01-18 02:55:49\n",
      "42 2023-01-18 03:04:31\n",
      "43 2023-01-18 03:15:28\n",
      "44 2023-01-18 03:19:05\n",
      "45 2023-01-18 03:22:08\n",
      "46 2023-01-18 03:23:16\n",
      "47 2023-01-18 03:28:02\n",
      "48 2023-01-18 03:29:28\n",
      "49 2023-01-18 03:37:00\n",
      "50 2023-01-18 03:37:41\n",
      "51 2023-01-18 03:43:07\n",
      "52 2023-01-18 03:43:55\n",
      "53 2023-01-18 03:51:00\n",
      "54 2023-01-18 03:57:13\n",
      "55 2023-01-18 03:57:13\n",
      "56 2023-01-18 03:59:12\n",
      "57 2023-01-18 04:00:21\n",
      "58 2023-01-18 04:00:38\n",
      "59 2023-01-18 04:18:52\n",
      "60 2023-01-18 04:41:22\n",
      "61 2023-01-18 04:54:08\n",
      "62 2023-01-18 04:55:05\n",
      "63 2023-01-18 05:00:37\n",
      "64 2023-01-18 05:03:57\n",
      "65 2023-01-18 05:09:55\n",
      "66 2023-01-18 06:13:17\n",
      "67 2023-01-18 06:16:39\n",
      "68 2023-01-18 07:33:47\n",
      "69 2023-01-18 07:50:22\n",
      "70 2023-01-18 08:20:26\n",
      "71 2023-01-18 08:24:54\n",
      "72 2023-01-18 08:28:46\n",
      "73 2023-01-18 10:47:30\n",
      "74 2023-01-18 10:53:25\n",
      "75 2023-01-18 10:55:33\n",
      "76 2023-01-18 11:00:22\n",
      "77 2023-01-18 11:01:37\n",
      "78 2023-01-18 11:12:46\n",
      "79 2023-01-18 11:53:38\n",
      "80 2023-01-18 12:07:16\n",
      "81 2023-01-18 12:09:23\n",
      "82 2023-01-18 12:13:24\n",
      "83 2023-01-18 12:14:28\n",
      "84 2023-01-18 12:15:45\n",
      "85 2023-01-18 12:17:11\n",
      "86 2023-01-18 12:27:53\n",
      "87 2023-01-18 12:45:48\n",
      "88 2023-01-18 12:52:33\n",
      "89 2023-01-18 12:53:12\n",
      "90 2023-01-18 13:00:46\n",
      "91 2023-01-18 13:01:43\n",
      "92 2023-01-18 13:04:22\n",
      "93 2023-01-18 13:11:45\n",
      "94 2023-01-18 13:22:18\n",
      "95 2023-01-18 13:23:49\n",
      "96 2023-01-18 13:33:38\n",
      "97 2023-01-18 13:40:31\n",
      "98 2023-01-18 13:44:33\n",
      "99 2023-01-18 13:44:58\n",
      "100 2023-01-18 13:50:13\n",
      "101 2023-01-18 13:53:59\n",
      "102 2023-01-18 13:54:06\n",
      "103 2023-01-18 13:55:05\n",
      "104 2023-01-18 13:56:32\n",
      "105 2023-01-18 13:58:45\n",
      "106 2023-01-18 14:00:33\n",
      "107 2023-01-18 14:01:10\n",
      "108 2023-01-18 14:01:10\n",
      "109 2023-01-18 14:03:11\n",
      "110 2023-01-18 14:05:57\n",
      "111 2023-01-18 14:06:38\n",
      "112 2023-01-18 14:11:27\n",
      "113 2023-01-18 14:22:51\n",
      "114 2023-01-18 14:24:55\n",
      "115 2023-01-18 14:27:11\n",
      "116 2023-01-18 14:34:08\n",
      "117 2023-01-18 14:37:49\n",
      "118 2023-01-18 14:41:11\n",
      "119 2023-01-18 14:42:24\n",
      "120 2023-01-18 14:42:42\n",
      "121 2023-01-18 14:44:21\n",
      "122 2023-01-18 14:54:17\n",
      "123 2023-01-18 14:59:06\n",
      "124 2023-01-18 15:02:02\n",
      "125 2023-01-18 15:07:30\n",
      "126 2023-01-18 15:08:33\n",
      "127 2023-01-18 15:09:26\n",
      "128 2023-01-18 15:13:16\n",
      "129 2023-01-18 15:20:09\n",
      "130 2023-01-18 15:24:41\n",
      "131 2023-01-18 15:29:39\n",
      "132 2023-01-18 15:41:28\n",
      "133 2023-01-18 15:42:24\n",
      "134 2023-01-18 15:42:53\n",
      "135 2023-01-18 15:43:16\n",
      "136 2023-01-18 15:44:06\n",
      "137 2023-01-18 15:48:41\n",
      "138 2023-01-18 15:49:32\n",
      "139 2023-01-18 15:58:05\n",
      "140 2023-01-18 15:59:55\n",
      "141 2023-01-18 15:59:56\n",
      "142 2023-01-18 16:01:40\n",
      "143 2023-01-18 16:16:52\n",
      "144 2023-01-18 16:17:40\n",
      "145 2023-01-18 16:23:38\n",
      "146 2023-01-18 16:27:09\n",
      "147 2023-01-18 16:30:45\n",
      "148 2023-01-18 16:31:05\n",
      "149 2023-01-18 16:31:09\n",
      "150 2023-01-18 16:32:14\n",
      "151 2023-01-18 16:33:55\n",
      "152 2023-01-18 16:38:51\n",
      "153 2023-01-18 16:39:27\n",
      "154 2023-01-18 16:40:06\n",
      "155 2023-01-18 16:42:43\n",
      "156 2023-01-18 16:45:03\n",
      "157 2023-01-18 16:48:31\n",
      "158 2023-01-18 16:58:57\n",
      "159 2023-01-18 17:11:43\n",
      "160 2023-01-18 17:21:11\n",
      "161 2023-01-18 17:21:48\n",
      "162 2023-01-18 17:33:41\n",
      "163 2023-01-18 17:33:43\n",
      "164 2023-01-18 17:39:05\n",
      "165 2023-01-18 17:54:30\n",
      "166 2023-01-18 17:57:38\n",
      "167 2023-01-18 17:58:25\n",
      "168 2023-01-18 18:05:17\n",
      "169 2023-01-18 18:07:02\n",
      "170 2023-01-18 18:09:27\n",
      "171 2023-01-18 18:12:21\n",
      "172 2023-01-18 18:17:54\n",
      "173 2023-01-18 18:18:59\n",
      "174 2023-01-18 18:21:27\n",
      "175 2023-01-18 18:21:35\n",
      "176 2023-01-18 18:32:35\n",
      "177 2023-01-18 18:35:27\n",
      "178 2023-01-18 18:57:17\n",
      "179 2023-01-18 18:59:01\n",
      "180 2023-01-18 19:09:44\n",
      "181 2023-01-18 19:17:56\n",
      "182 2023-01-18 19:20:11\n",
      "183 2023-01-18 19:23:35\n",
      "184 2023-01-18 19:46:10\n",
      "185 2023-01-18 19:50:17\n",
      "186 2023-01-18 19:56:03\n",
      "187 2023-01-18 19:58:39\n",
      "188 2023-01-18 20:00:28\n",
      "189 2023-01-18 20:09:34\n",
      "190 2023-01-18 20:19:45\n",
      "191 2023-01-18 20:22:30\n",
      "192 2023-01-18 20:28:34\n",
      "193 2023-01-18 20:44:35\n",
      "194 2023-01-18 21:00:26\n",
      "195 2023-01-18 21:06:57\n",
      "196 2023-01-18 21:14:25\n",
      "197 2023-01-18 21:27:51\n",
      "198 2023-01-18 21:33:50\n",
      "199 2023-01-18 21:33:57\n",
      "200 2023-01-18 21:37:42\n",
      "201 2023-01-18 21:40:50\n",
      "202 2023-01-18 21:45:21\n",
      "203 2023-01-18 22:01:28\n",
      "204 2023-01-18 22:15:39\n",
      "205 2023-01-18 22:19:40\n",
      "206 2023-01-18 22:39:19\n",
      "207 2023-01-18 22:44:50\n",
      "208 2023-01-18 22:51:05\n",
      "209 2023-01-18 23:03:18\n",
      "210 2023-01-18 23:16:01\n",
      "211 2023-01-18 23:28:36\n",
      "212 2023-01-18 23:37:10\n",
      "213 2023-01-18 23:38:35\n",
      "214 2023-01-18 23:42:38\n",
      "215 2023-01-18 23:48:33\n",
      "216 2023-01-18 23:52:54\n",
      "217 2023-01-18 23:57:58\n",
      "operación exitosa\n",
      "{'#covid19': 6, '#mexico': 6, '#cronicaspostcuarentena': 4, '#angulo7informa': 3, '#covid': 3, '#reportaje': 2, '#pandemia': 2, '#elotroangulo': 2, '#periodismodeinvestigacion': 2, '#tbt': 2, '#epilepsia': 2, '#influenza': 2, '#puebla': 2, '#forocuatro': 2, '#forocuatrotv': 2, '#hermosillo': 2, '#sonora': 2, '#slp': 2, '#slplife': 2, '#cp78233': 2, '@hlgatell': 35, '@conacyt_mx': 23, '@popcrush': 15, '@britneyjeanicon': 12, '@lopezobrador': 10, '@anqelicbritney': 9, '@saludgobpue': 4, '@jantoniomtzga': 4, '@samuel_garcias': 3, '@claudiashein': 3, '@edd_eddiiy': 3, '@summers_tvs': 3, '@delfinagomeza': 2, '@tu_imss': 2, '@zoerobledo': 2, '@ssalud_mx': 2, '@vicentefoxque': 2, '@lillytellez': 2, '@alfredodelmazo': 2, 'crisis': 34, 'pandemia': 26, 'vacuna': 17, 'años': 11, 'tratamiento': 10, '️': 9, 'vacunas': 8, 'mexico': 7, '_': 7, 'mejor': 7, 'guerra': 7, 'dias': 7, 'prueba': 7, 'tipo': 6, 'sintomas': 6, 'vez': 6, 'vida': 6, 'salud': 6, '😂': 6, 'igual': 5, 'nivel': 5, 'casos': 5, 'linea': 5, '💉': 5, 'solo': 4, 'corazon': 4, 'agua': 4, 'paso': 4, 'sintoma': 4, 'niños': 4, 'personas': 4, 'metro': 4, 'tambien': 4, 'mundo': 4, 'covid': 4, 'influenza': 4, 'moderna': 4, 'dia': 4, '😍': 4, 'enero': 4, 'also': 4, 'datos': 4, '”': 3, 'gato': 3, 'mundial': 3, 'animal': 3, 'encierro': 3, 'problemas': 3, 'asesino': 3, 'millones': 3, 'grandes': 3, 'amigo': 3, 'cartera': 3, 'desarrollo': 3, 'mexicanos': 3, 'gracias': 3, 'miedo': 3, 'contingencia': 3, 'tiempos': 3, 'permiso': 3, 'serio': 3, 'marca': 3, 'primera': 3, 'refuerzo': 3, 'amlo': 3, 'tenia': 3, '☕': 3, 'democracia': 3, 'derecho': 3, 'sistema': 3, 'paises': 3, 'estado': 3, 'navidad': 3, 'despues': 3, '😜': 3, 'gobiernos': 3, 'tren': 3, 'red': 3, 'buenos': 3, 'medica': 3, 'gobierno': 3, 'acuerdo': 3, 'meses': 3, 'nuevo': 3, 'cosas': 3, 'll': 3, 'secretario': 3, 'unico': 3, 'medidas': 3, 'covid-19': 3, 'you': 3, 'enfermedad': 3, 'have': 3, 'evidencia': 3, 'negativos': 3, '“': 2, 'info': 2, 'casa': 2, 'frente': 2, 'familias': 2, 'cuerpo': 2, 'vlv': 2, 'incondicional': 2, 'necesitas': 2, 'irte': 2, 'vacaciones': 2, 'embarque': 2, 'pension': 2, 'prefieres': 2, 'rescatistas': 2, 'alma': 2, 'tu': 2, 'diciembre': 2, 'estatal': 2, 'bueno': 2, 'excelente': 2, 'involucrados': 2, 'amor': 2, 'centro': 2, 'sexenio': 2, 'tiempo': 2, 'verdad': 2, 'ambiental': 2, 'jajaja': 2, 'ataques': 2, 'final': 2, 'clase': 2, 'medios': 2, 'interesante': 2, 'capacidad': 2, 'dosis': 2, 'unica': 2, 'opciones': 2, 'gobernador': 2, 'aumento': 2, 'fiscal': 2, 'gente': 2, 'republica': 2, 'ecatepec': 2, 'mejores': 2, 'tabaco': 2, 'marihuana': 2, 'modo': 2, 'enfermo': 2, 'proyecto': 2, 'video': 2, 'pase': 2, 'plena': 2, 'paramedico': 2, 'tratamientos': 2, 'ataque': 2, '🥳': 2, 'secretaria': 2, 'calidad': 2, 'enfermedades': 2, 'medicamentos': 2, 'prestamo': 2, 'destino': 2, 'gratuita': 2, 'positivos': 2, 'defunciones': 2, 'ultima': 2, 'nuevos': 2, 'trabajo': 2, 'contagios': 2, 'ademas': 2, 'sera': 2, 'estudio': 2, 'social': 2, 'amigos': 2, 'sanitarias': 2, 'china': 2, 'dr': 2, 'fuerte': 2, 'cajas': 2, 'medicamento': 2, 'especifico': 2, 'caracteristicas': 2, 'actual': 2, 'evidence': 2, 'hidalgo': 2, 'importante': 2, 'cambio': 2, 'primero': 2, 'colonia': 2, 'it': 2, '👍': 2, '👁': 2, 'doesn’t': 2, 'virus': 2, 'detras': 2, 'cafe': 2, 'sanitaria': 2, 'suministro': 2, 'diarios': 2, 'miercoles': 2, 'pm': 2, 'that’s': 2, 'that': 2, 'decreto': 2, 'fracaso': 2, 'emocional': 2, 'calling': 2, 'malo': 2, 'alergia': 2, 'ansiedad': 2, 'falta': 2, 'quimios': 2, 'economica': 2, '🏻': 2, 'sitio': 2, 'web': 2, 'trabajadores': 2, 'incapacidad': 2, 'dios': 2, 'cura': 2, 'incertidumbre': 2, 'abundancia': 2, 'programa': 2, 'dañas': 2, 'manera': 2, 'resultados': 2, 'dolor': 2, 'hospitales': 2, '-asintomaticas': 2, 'modernas': 2, 'adicionales': 2, 'pareja': 2, 'masculina': 2, 'asintomatica': 2, '-mujer': 2, 'cultivo': 2, 'positivo': 2, 'candida': 2, 'luz': 2, 'complicada': 2, 'año': 2, 'mitad': 2, 'falsos': 2, 'asintomaticos': 2, 'cuidense': 2, 'sentimientos': 2, 'directa': 2, 'jubilados': 2}\n"
     ]
    }
   ],
   "source": [
    "words = ejecutarAnalisis(tweets)\n",
    "print(words)\n",
    "#%pip  install es_core_news_sm\n",
    "\n",
    "#se observa que hace un conteo de todos los hashtagas, menciones y palabras; sin embargo, solo se muestran 5 en la página\n",
    "#falta tener la versión correcta de spacy y es_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para iniciar tabla de localidades, sin polígono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO localidad (nombre,id_loc_padre) VALUES ('Aguascalientes',1),('Baja California',1),('Baja California Sur',1),('Campeche',1),('Ciudad de México',1),('Chihuahua',1),('Chiapas',1),('Coahuila',1),('Colima',1),('Durango',1),('Guanajuato',1),('Guerrero',1),('Hidalgo',1),('Jalisco',1),('Estado de México',1),('Michoacán',1),('Morelos',1),('Nayarit',1),('Nuevo León',1),('Oaxaca',1),('Puebla',1),('Querétaro',1),('Quintana Roo',1),('San Luis Potosí',1),('Sinaloa',1),('Sonora',1),('Tabasco',1),('Tamaulipas',1),('Tlaxcala',1),('Veracruz',1),('Yucatán',1),('Zacatecas',1);\n"
     ]
    }
   ],
   "source": [
    "#Para crear desde cero la tabla de localidades\n",
    "sql =\"INSERT INTO localidad (nombre) VALUES ('México');\"\n",
    "#db.update(sql)\n",
    "\n",
    "#se emplea archivo anterior de referencia\n",
    "with open(\"/home/adrian/Miopers/home/data/map_hashtags_hist.geojson\",\"r\") as json_file:\n",
    "    EdosPol = json.load(json_file)\n",
    "listEdo = list(EdosPol.keys())\n",
    "locPadre = \"México\"\n",
    "sql = \"SELECT id_localidad FROM localidad WHERE nombre LIKE '{}';\".format(locPadre)\n",
    "idLocPadre = db.selectOne(sql)\n",
    "\n",
    "sql = \"INSERT INTO localidad (nombre,id_loc_padre) VALUES \"\n",
    "for  i in listEdo:\n",
    "    comp = \"('{}',{}),\".format(i, idLocPadre)\n",
    "    sql = sql + comp\n",
    "    \n",
    "sql = sql[: -1]\n",
    "sql = sql + \";\"\n",
    "\n",
    "print(sql)\n",
    "#db.update(sql)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mexico hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf8\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta, date\n",
    "#from estados import estados\n",
    "import collections\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import urllib\n",
    "\"\"\"\n",
    "@Miopers-HOME - mexico_hashtags.py\n",
    "Módulo de Python que permite poder realizar el analísis de los hashtag por medio del analísis de expresinoes regulares y otros \n",
    "métodos y la creación de un archivo .json donde se almacenan la información de los hashtags almacenados. Esto con el objetivo de \n",
    "almacenr la información y poder utilizarla después en el analisis y gráficas de miopers. \n",
    "Date: March-2021\n",
    "Tipo de Documentación: Google Docstrings\n",
    "\"\"\"\n",
    "\n",
    "def getText(tweet):\n",
    "    \"\"\"\n",
    "    Summary: Función que permite extraer la información de los tweets descargados desde Monto para ser procesados.\n",
    "        Los textos de los tweets pueden ser descargados de forma total, truncada o el texto completo, dependiendo de\n",
    "        la calidad del tweet descargado. \n",
    "    Args:\n",
    "        tweet: Variable para identificar el texto de cada uno de los tweets descargados\n",
    "\n",
    "    Returns:\n",
    "        list: Se devuelve una lista con la información del texto de todos los tweets, estos pueden ser\n",
    "        de forma total o de forma parcial, dependiendo de la condición del tweet\n",
    "\n",
    "    \"\"\"\n",
    "    return tweet['text'] if not tweet['truncated'] else tweet['extended_tweet']['full_text']\n",
    "\n",
    "def getDate(tweet):\n",
    "    \"\"\"\n",
    "    Summary: Función para poder extraer de los tweets la fecha de cuando fueron emitidos cada uno de estos.\n",
    "    \n",
    "    Args:\n",
    "        tweet: Variable para identificar el texto de cada uno de los tweets descargados\n",
    "\n",
    "    Returns:\n",
    "        Se devuelve la fecha de publicación de cada uno de los tweets en formato datetime. \n",
    "\n",
    "    \"\"\"\n",
    "    timestamp = tweet['timestamp_ms'][:-3]\n",
    "    return datetime.fromtimestamp(int(timestamp)) + timedelta(hours=5)\n",
    "\n",
    "def getPlace(data):\n",
    "    \"\"\"\n",
    "    Summary: Función que nos regresa el valor por de el lugar donde fue publicado el tweets (estado) para esto\n",
    "        se hace un split de la inofrmación que hay en el tweet pra poder extraer esta información en especifio de \n",
    "        todo el texto.\n",
    "\n",
    "    Args:\n",
    "        data: Parametro para identificar la ubicación del tweet filtrado de la función getDate() \n",
    "              donde lo que hacemos es una extracción de información en especifico.\n",
    "\n",
    "    Returns:\n",
    "        Se gresa la información del lugar donde fue publicado el tweets en caso contrario regresa un string para indicar\n",
    "        que se desconoce esa información. \n",
    "\n",
    "    \"\"\"\n",
    "    data = preprocess(data)\n",
    "    if len(data.split(',')) == 2:\n",
    "        return  data.split(',')[0].strip(),data.split(',')[1].strip()\n",
    "    else:\n",
    "        return '*','*'\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Summary: Procesamos la información y la limpiamos utilizando NLP donde removemos acentos, y extraemos\n",
    "        las palabras principales que nos interesan en el re.compile\n",
    "\n",
    "    Args: \n",
    "        text: El texto del tweet que estamos analizando \n",
    "\n",
    "    Returns:\n",
    "        Devuelve el texto pricesado sin acentos y con las palabras de interes extraidas de igual forma\n",
    "        en formato de texto\n",
    "\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('á','a')\n",
    "    text = text.replace('é','e')\n",
    "    text = text.replace('í','i')\n",
    "    text = text.replace('ó','o')\n",
    "    text = text.replace('ú','u')\n",
    "    text = text.replace('\\n','')\n",
    "    return text\n",
    "\n",
    "def ejecutarAnalisis(tweets ):\n",
    "    data_path = \"/home/adrian/Miopers/home/data/\"\n",
    "    #contexto = re.compile(r'(covid|(v|b)iru(s|z)|sars(-| )?cov|contingencia|sanitaria|sintoma|neumonia|quedateencasa|pandemia|encierro|cuarentena|encasa|cuandoestoseacabe|aislamientosocial|susana ?distancia)',re.I)\n",
    "\n",
    "    # Creamos diccinario para ir almacenaando la información de cada estado\n",
    "    # with open(data_path+\"map_hashtags_hist.geojson\",\"r\") as json_file:\n",
    "    #     map = json.load(json_file)\n",
    "    map = dict()\n",
    "    # Cronometro para medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Creamos diccionario para almacenar información relativa a las fechas de los tweets\n",
    "    results = dict()\n",
    "    estados = json.loads(db.getAnalisisDicc(\"hashtags\")) #caraga estados desde la base de datos\n",
    "    hashtags = []\n",
    "    # Vamos recorriendo la lista de cada uno de los tweets consultados y los procesamos de acuerdo a las anteriore funciones\n",
    "    for i,tweet in enumerate(tweets):\n",
    "        text = preprocess(getText(tweet))\n",
    "        date = getDate(tweet).date()\n",
    "        #print('Fecha: ' + str(getDate(tweet)))\n",
    "\n",
    "\n",
    "        place0,place1 = getPlace(tweet['place']['full_name']) #recibe '*' en caso de que no venga el lugar\n",
    "        # Buscamos y desplegamos la información de los tweets por cada uno de los estados de estados.py\n",
    "        for state in estados:\n",
    "            # Se conoce el estado\n",
    "            if place0 != '*':\n",
    "                if place0 in estados[state] or place1 in estados[state]:\n",
    "                    aux = map.get(state,{}) # mapeamos la información de cada uno de los estados\n",
    "                    for word in text.split(' '):\n",
    "                        if word.startswith('#'): #Buscamos la información de las palabras que inician en hashtag\n",
    "                            if word not in hashtags:\n",
    "                                hashtags.append(word)\n",
    "                            counter = aux.get(word,0)\n",
    "                            aux[word] = counter + 1\n",
    "\n",
    "                    #counter = aux.get('total',0)\n",
    "                    #aux['total'] = counter + 1\n",
    "                    #aqui se guarda la cuenta de todos los hashtags del estado\n",
    "                    map[state] = aux\n",
    "                    break\n",
    "                # Si es el ultimo estado y el campo 1 es mexico se pasa a Estado de Mexico\n",
    "                elif state == 'Zacatecas' and place1 == 'mexico':\n",
    "                    aux = map.get('Estado de México',{})\n",
    "                    for word in text.split(' '):\n",
    "                        if word.startswith('#'):\n",
    "                            if word not in hashtags:\n",
    "                                hashtags.append(word)\n",
    "                            counter = aux.get(word,0)\n",
    "                            aux[word] = counter + 1\n",
    "                    # Se recorren todos los estados hasta llegar al Edo Mex.\n",
    "                    #counter = aux.get('total',0)\n",
    "                    #aux['total'] = counter + 1\n",
    "                    map['Estado de México'] = aux\n",
    "\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    ####\n",
    "    #arr = list(dataDict.keys())\n",
    "    print(\"longitud hashtagas\", len(hashtags))\n",
    "    wordsDB = db.getAllAnalisisTopic(\"hashtags\")\n",
    "    listaPal = []\n",
    "\n",
    "    for i in wordsDB:\n",
    "        listaPal.append(i[1])\n",
    "\n",
    "    id_analisis = db.getIdAnalisis(\"hashtags\")\n",
    "    #print(id_analisis)\n",
    "    cent = False\n",
    "\n",
    "\n",
    "    sql = \"INSERT INTO analisis_detalle (id_analisis, descripcion) VALUES \"\n",
    "    for  i in hashtags:\n",
    "        if i not in listaPal:\n",
    "            cent =True\n",
    "            comp = \"({},'{}'),\".format(id_analisis, i)\n",
    "            sql = sql + comp\n",
    "        \n",
    "    sql = sql[: -1]\n",
    "    sql = sql + \";\"\n",
    "\n",
    "    #print(sql)\n",
    "    if cent:\n",
    "         db.update(sql)\n",
    "\n",
    "    #para ingresar los datos a la base de datos\n",
    "        \n",
    "    id_analisis = db.getIdAnalisis(\"hashtags\")\n",
    "    id_proceso = db.getCurrentProceso()\n",
    "    wordsDB = db.getAllAnalisisTopic(\"hashtags\")\n",
    "    \n",
    "    hashtag_id = dict()\n",
    "    for i in wordsDB:\n",
    "        hashtag_id[i[1]]=i[0]\n",
    "    \n",
    "    sql = \"INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES \"\n",
    "    edos = list(map)\n",
    "    for estado in edos:\n",
    "        id_localidad = db.getIDLoc(estado)\n",
    "        for hashtag in map[estado].keys():\n",
    "            #print(estado,hashtag,map[estado][hashtag])\n",
    "            comp = \"({},{},{},{},{}),\".format(hashtag_id[hashtag],id_analisis,id_proceso,map[estado][hashtag],id_localidad)\n",
    "            sql = sql + comp\n",
    "        \n",
    "    sql = sql[: -1]\n",
    "    sql = sql + \";\"\n",
    "\n",
    "    \n",
    "    print(sql)\n",
    "        \n",
    "        \n",
    "    db.update(sql)\n",
    "    \n",
    "    \n",
    "    ####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return map\n",
    "    # Creamos un diccionario pra guardar la información relativa de los estados y sus hastags\n",
    "    data = dict()\n",
    "\n",
    "    # Recorremos con un loop para cada uno de los estados gauardados en el mexico.geojson\n",
    "    for state in map:\n",
    "        aux = dict()\n",
    "        # Buscamos los hastags hallados en cada uno de los estados y si es de longitud mayor a 3 los guardamos en el dic. data\n",
    "        for i, hashtag in enumerate(reversed(sorted(map[state].items(), key = lambda item:item[1]))):\n",
    "            if i > 3: #aqui limitamos para solo recorrer el TOP 3 de hashtags mas frecuentes\n",
    "                break\n",
    "            # Recorremos y los hashtags se van guardando dentro del diccionario\n",
    "            aux[hashtag[0]] = hashtag[1]\n",
    "        # Al final de cuentas tenemos un diccionario de diccionarios de hashtags.    \n",
    "        data[state] = aux \n",
    "\n",
    "    \n",
    "    # Rellenamos el diccionario con los datetime de cuando fueron procesados\n",
    "    #results['time'] = datetime.today().ctime()\n",
    "        \n",
    "    # Trabajamos con el archivo mexico.geojson y las coordenadas para la creación del mapa interactivo y los tweets que almacena. \n",
    "    # with open(data_path+'mexico.geojson', encoding=\"utf-8\") as f:\n",
    "    #     geoMap = json.load(f)\n",
    "    #     # Dentro del diccionario almacenamos el tipo de estado y las cordenadas (crs)\n",
    "    #     results['type'] = geoMap['type']\n",
    "    #     results['crs'] = geoMap['crs']\n",
    "    #     states = list() # Inicializamos un diccionario\n",
    "    #     for state in geoMap['features']:\n",
    "    #         # Leemos la inforamción de cada uno de los estados en y en el diccionario guardamos la información de el nombre del estado y coordenadas\n",
    "    #         #state['properties']['data'] = data[state['properties']['name']]\n",
    "    #         state['properties']['data'] = data.get(state['properties']['name'], {\"total\":0})\n",
    "    #         states.append(state)\n",
    "    #     results['features'] = states\n",
    "\n",
    "\n",
    "    # Creamos en un directorio para poder guardar en un archivo .json la información de los hastags analyzados en este script\n",
    "    # with open('/home/adrian/Miopers/web/src/data/map_hashtags_result.json', 'w') as f:\n",
    "    #     json.dump(results, f)\n",
    "    # with open(data_path+'map_hashtags_hist.geojson', 'w') as f:\n",
    "    #     json.dump(map, f)\n",
    "    #subir JSON al repo\n",
    "    #import IO_json\n",
    "    #IO_json.upload_json(\"home/data/map_hashtags_hist.geojson\",map)\n",
    "    #IO_json.upload_json(\"home/data/map_hashtags_result.geojson\",results)\n",
    "\n",
    "    '''\n",
    "    Si se desean obtener los hashtags por estado, descomentar las siguientes lineas\n",
    "    with open('all_hashtags.json', 'w') as f:\n",
    "        json.dump(map, f)\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Mapa de méxico actualizado\\n--- %s segundos \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud hashtagas 62\n",
      "operación exitosa\n",
      "INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES (1021,7,18,1,6),(1592,7,18,1,6),(1011,7,18,2,6),(1593,7,18,1,6),(1214,7,18,1,6),(1594,7,18,1,6),(1595,7,18,1,6),(1596,7,18,1,6),(1598,7,18,1,6),(1599,7,18,1,6),(1600,7,18,1,6),(1601,7,18,1,6),(1602,7,18,1,6),(1603,7,18,1,6),(1604,7,18,1,6),(1607,7,18,2,6),(1608,7,18,2,6),(1339,7,18,1,6),(1609,7,18,1,6),(1610,7,18,1,6),(1611,7,18,1,6),(1635,7,18,1,6),(1638,7,18,1,6),(1587,7,18,1,29),(1011,7,18,1,29),(1585,7,18,1,23),(1586,7,18,1,23),(1593,7,18,1,23),(1597,7,18,1,23),(1612,7,18,1,23),(1613,7,18,1,23),(1614,7,18,1,23),(1615,7,18,1,23),(1616,7,18,1,23),(1617,7,18,1,23),(1618,7,18,1,23),(1619,7,18,1,23),(1620,7,18,1,23),(1621,7,18,1,23),(1622,7,18,1,23),(1629,7,18,1,3),(1208,7,18,1,17),(1588,7,18,1,20),(1605,7,18,1,20),(1626,7,18,1,20),(1636,7,18,1,20),(1637,7,18,1,31),(1589,7,18,1,8),(1590,7,18,1,8),(1591,7,18,1,8),(1207,7,18,2,27),(1606,7,18,1,27),(1623,7,18,3,27),(1624,7,18,1,27),(1037,7,18,3,27),(1462,7,18,3,27),(1625,7,18,2,27),(1627,7,18,1,13),(1628,7,18,1,13),(1630,7,18,1,2),(1631,7,18,1,2),(1632,7,18,1,2),(1633,7,18,1,2),(1634,7,18,1,2);\n",
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "nmap = ejecutarAnalisis(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciudad de México #cdmx🇲🇽 2\n",
      "Ciudad de México #cdmx 2\n",
      "Jalisco #austeridadhomicidaenelmetro 2\n"
     ]
    }
   ],
   "source": [
    "def imprime (map ):\n",
    "    hashtag = []\n",
    "    edos = list(map)\n",
    "    for i in edos:\n",
    "        for e in map[i].keys():\n",
    "            if e not in hashtag:\n",
    "                hashtag.append(e)\n",
    "            if map[i][e] > 1:\n",
    "                print(i,e,map[i][e])\n",
    "\n",
    "imprime(nmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/adrian/Miopers/baseDatos/EditarBase.ipynb Celda 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adrian/Miopers/baseDatos/EditarBase.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(nmap)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nmap' is not defined"
     ]
    }
   ],
   "source": [
    "len(nmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/home/adrian/Miopers/home/data/map_hashtags_hist.geojson\",\"r\") as json_file:\n",
    "    EdosPol = json.load(json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis síntomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO localidad (nombre,id_loc_padre) VALUES ('Azcapotzalco',6),('Iztacalco',6),('Álvaro Obregón',6),('Xochimilco',6),('Venustiano Carranza',6),('Tlalpan',6),('Cuajimalpa de Morelos',6),('Cuauhtémoc',6),('Iztapalapa',6),('Milpa Alta',6),('Benito Juárez',6),('Gustavo A. Madero',6),('Coyoacán',6),('Miguel Hidalgo',6),('La Magdalena Contreras',6),('Tláhuac',6);\n",
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#para agregar alcaldias\n",
    "\n",
    "with open(\"/home/adrian/Miopers/sintomas/data/json/alcaldias.json\",\"r\") as json_file:\n",
    "    alcaldias = json.load(json_file)\n",
    "#listEdo = list(EdosPol.keys())\n",
    "locPadre = \"Ciudad de México\"\n",
    "sql = \"SELECT id_localidad FROM localidad WHERE nombre LIKE '{}';\".format(locPadre)\n",
    "idLocPadre = db.selectOne(sql)\n",
    "\n",
    "sql = \"INSERT INTO localidad (nombre,id_loc_padre) VALUES \"\n",
    "for  i in alcaldias['features']:\n",
    "    comp = \"('{}',{}),\".format(i['properties']['nomgeo'], idLocPadre)\n",
    "    sql = sql + comp\n",
    "    \n",
    "sql = sql[: -1]\n",
    "sql = sql + \";\"\n",
    "\n",
    "print(sql)\n",
    "#db.update(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO analisis_detalle (id_analisis, descripcion) VALUES (8,'sintomas'),(8,'mentales');\n",
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "#insertar obervaciones de analisis de sintomas\n",
    "id_analisis = db.getIdAnalisis(\"sintomas\")\n",
    "sintomas = \"sintomas\"\n",
    "mentales = \"mentales\"\n",
    "sql = \"INSERT INTO analisis_detalle (id_analisis, descripcion) VALUES ({},'{}'),({},'{}');\".format(id_analisis,sintomas,id_analisis,mentales)\n",
    "print(sql)\n",
    "#db.update(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataImport(filename):\n",
    "    info = []\n",
    "    for xs in filename:\n",
    "        data = open(xs)\n",
    "        for r in data:\n",
    "            r = r.replace('\\n', '')\n",
    "            r = re.sub('https\\W*t.co/\\w*', '', r)\n",
    "            info.append(r.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics_b as mt\n",
    "import downTweets_b as dt\n",
    "import data_json_b as dj\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# def dataImport(filename):\n",
    "#     info = []\n",
    "#     for xs in filename:\n",
    "#         data = open(xs)\n",
    "#         for r in data:\n",
    "#             r = r.replace('\\n', '')\n",
    "#             r = re.sub('https\\W*t.co/\\w*', '', r)\n",
    "#             info.append(r.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "#     return info\n",
    "\n",
    "def dataImport(diccWords) -> List[str]:\n",
    "    \"\"\"lee una lista de palabras, cada renglón es un elemento de la lista resultante\"\"\"\n",
    "    tokens = []\n",
    "    for word in diccWords.split('\\n'):\n",
    "        word = re.sub('https\\W*t.co/\\w*', '', word)\n",
    "        tokens.append(word.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "    return tokens\n",
    "\n",
    "def ejecutarAnalisis(tweets):\n",
    "    try: #importa los diccionarios\n",
    "        sql = \"SELECT diccionario_tema FROM temas  WHERE tema LIKE 'COVID';\"\n",
    "        ndic = db.selectOne(sql) #para modificar cuando se convierta a class\n",
    "        filtro = dataImport(ndic)\n",
    "        dicc = json.loads(db.getAnalisisDicc('sintomas'))\n",
    "        sintomas = dicc['sintomas']\n",
    "        mentales = dicc['mentales']\n",
    "        # direccion = data_path+'json/'\n",
    "        # mapa_alcaldias = direccion+'alcaldias.json'\n",
    "        # feed = data_path+\"json/feed.pickle\"\n",
    "        # feed_json = data_path+\"json/feed.json\"\n",
    "    except :\n",
    "        print(\"Error, Archivos no encontrados\")\n",
    "    today = datetime.now()\n",
    "    dth = today.strftime('%a %b %d %H:%M:%S %Y')\n",
    "    \n",
    "    ####\n",
    "    #return filtro, sintomas, mentales\n",
    "    ####\n",
    "\n",
    "\n",
    "    #try:\n",
    "    #print(filtro, '\\n',tweets[:10])\n",
    "    dataFull = dt.downloadData(tweets,filtro)\n",
    "    data = mt.ReturnDelegacion(dataFull[0])\n",
    "    \n",
    "    datam = mt.etiqueta(data, mentales)\n",
    "    data_mp = datam[datam['etiqueta'] == 1]\n",
    "    twe = data_mp['id'].tolist()\n",
    "    data_p = mt.rept_clase(data_mp, 'mentales')\n",
    "    data_f = mt.agrupaFecha(data_mp)\n",
    "    \n",
    "\n",
    "    dataS = mt.etiqueta(data, sintomas)\n",
    "    data_Sp = dataS[dataS['etiqueta'] == 1]\n",
    "    twe2 = data_Sp['id'].tolist()\n",
    "    data_ps = mt.rept_clase(data_Sp, 'sintomas')\n",
    "    data_f2 = mt.agrupaFecha(data_Sp)\n",
    "    \n",
    "\n",
    "    # total = pd.merge(data_f, data_f2, on='fecha2')\n",
    "    # #return  mt.unionData(data_ps, data_p)\n",
    "    # total.to_csv(data_path+'data_timeline.csv')\n",
    " \n",
    "    # dj.json_timeline(direccion+'timeline', total, dth) #esta función sobreescribe el JSON con la nueva info\n",
    "    # dj.procesar_mapa2(direccion+'alcaldias', mt.unionData(data_ps, data_p), mapa_alcaldias, dth)\n",
    "    try:\n",
    "        dj.list_id(twe+twe2, feed)\n",
    "        dj.convert_pickle(feed, feed_json)\n",
    "    except:\n",
    "        print(\"Hubo algún error con el pickle\")\n",
    "    #return  mt.unionData(data_ps, data_p)\n",
    "    print('Analisis Sintomas terminado')\n",
    "    \n",
    "    #return data_mp, data_Sp\n",
    "\n",
    "    ##para agregar todos los resultados de sintomas mentales y físicos\n",
    "\n",
    "\n",
    "    id_locPadre = db.getIDLoc(\"Ciudad de México\")\n",
    "    alcaldias = db.getAllLocChild(id_locPadre)\n",
    "\n",
    "    resDel = dict()\n",
    "    for t in data_Sp['delegacion']:\n",
    "        aux = resDel.get(t,0)\n",
    "        aux = aux + 1\n",
    "        resDel[t] = aux\n",
    "\n",
    "    id_analisis = db.getIdAnalisis(\"sintomas\")\n",
    "    id_proceso = db.getCurrentProceso()\n",
    "\n",
    "    sql = \"INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES \"\n",
    "\n",
    "    analisisTema = db.getAllAnalisisTopic(\"sintomas\")\n",
    "\n",
    "    for  t in resDel:\n",
    "        sql2 = \"SELECT id_localidad FROM  localidad WHERE id_loc_padre = {} AND nombre LIKE '{}';\".format(id_locPadre,t)\n",
    "        comp = \"({},{},{},{},{}),\".format(analisisTema[0][0],id_analisis,id_proceso,resDel[t],alcaldias[t])\n",
    "        sql = sql + comp\n",
    "\n",
    "    resDel = dict()\n",
    "    for t in data_mp['delegacion']:\n",
    "        aux = resDel.get(t,0)\n",
    "        aux = aux + 1\n",
    "        resDel[t] = aux\n",
    "    for  t in resDel:\n",
    "        sql2 = \"SELECT id_localidad FROM  localidad WHERE id_loc_padre = {} AND nombre LIKE '{}';\".format(id_locPadre,t)\n",
    "        comp = \"({},{},{},{},{}),\".format(analisisTema[1][0],id_analisis,id_proceso,resDel[t],alcaldias[t])\n",
    "        sql = sql + comp\n",
    "\n",
    "    sql = sql[: -1]\n",
    "    sql = sql + \";\"\n",
    "\n",
    "    print(sql)\n",
    "    db.update(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:182: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf.append(df.loc[[i]])\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:183: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  newdf = newdf.append(df.iloc[[i]], ignore_index=True)\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:240: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['fecha2'] = fecha2\n",
      "/home/adrian/Miopers/baseDatos/metrics_b.py:240: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['fecha2'] = fecha2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisis Sintomas terminado\n"
     ]
    }
   ],
   "source": [
    "ejecutarAnalisis(tweets)\n",
    "#totm, tots = ejecutarAnalisis(tweets)\n",
    "#tots = ejecutarAnalisis(tweets, '/home/adrian/Miopers/sintomas/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Iztacalco': 1, 'Tlalpan': 1, 'Benito Juárez': 3, 'Cuajimalpa de Morelos': 1, 'Coyoacán': 3, 'Álvaro Obregón': 3, 'Iztapalapa': 2, 'Gustavo A. Madero': 1, 'Cuauhtémoc': 1}\n",
      "INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES (1771,8,19,1,35),(1771,8,19,1,39),(1771,8,19,3,44),(1771,8,19,1,40),(1771,8,19,3,46),(1771,8,19,3,36),(1771,8,19,2,42),(1771,8,19,1,45),(1771,8,19,1,41),(1772,8,19,3,44),(1772,8,19,1,39),(1772,8,19,1,35),(1772,8,19,1,46),(1772,8,19,2,45),(1772,8,19,1,38),(1772,8,19,2,42),(1772,8,19,1,36);\n",
      "operación exitosa\n"
     ]
    }
   ],
   "source": [
    "##para agregar todos los resultados de sintomas mentales y físicos\n",
    "\n",
    "\n",
    "id_locPadre = db.getIDLoc(\"Ciudad de México\")\n",
    "alacaldias = db.getAllLocChild(id_locPadre)\n",
    "\n",
    "resDel = dict()\n",
    "for t in tots['delegacion']:\n",
    "    aux = resDel.get(t,0)\n",
    "    aux = aux + 1\n",
    "    resDel[t] = aux\n",
    "\n",
    "id_analisis = db.getIdAnalisis(\"sintomas\")\n",
    "id_proceso = db.getCurrentProceso()\n",
    "\n",
    "sql = \"INSERT INTO resultados (id_detalle,id_analisis,id_proceso,cantidad,id_localidad) VALUES \"\n",
    "\n",
    "analisisTema = db.getAllAnalisisTopic(\"sintomas\")\n",
    "\n",
    "for  t in resDel:\n",
    "    sql2 = \"SELECT id_localidad FROM  localidad WHERE id_loc_padre = {} AND nombre LIKE '{}';\".format(id_locPadre,t)\n",
    "    comp = \"({},{},{},{},{}),\".format(analisisTema[0][0],id_analisis,id_proceso,resDel[t],alacaldias[t])\n",
    "    sql = sql + comp\n",
    "\n",
    "resDel = dict()\n",
    "for t in totm['delegacion']:\n",
    "    aux = resDel.get(t,0)\n",
    "    aux = aux + 1\n",
    "    resDel[t] = aux\n",
    "for  t in resDel:\n",
    "    sql2 = \"SELECT id_localidad FROM  localidad WHERE id_loc_padre = {} AND nombre LIKE '{}';\".format(id_locPadre,t)\n",
    "    comp = \"({},{},{},{},{}),\".format(analisisTema[1][0],id_analisis,id_proceso,resDel[t],alacaldias[t])\n",
    "    sql = sql + comp\n",
    "\n",
    "sql = sql[: -1]\n",
    "sql = sql + \";\"\n",
    "\n",
    "print(sql)\n",
    "db.update(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'created_at', 'id', 'id_str', 'text', 'display_text_range', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'extended_tweet', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tweets[0]\n",
    "t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 00:00:37-06:00\n",
      "1674021637221\n",
      "1674021637.221 2023-01-18 00:00:37.221000 1615589959540551680\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import downTweets_b as dt\n",
    "\n",
    "for t in tweets:\n",
    "    if t['id'] == 1615527133564002304:\n",
    "        da = (float(t['timestamp_ms'])/1000.0)\n",
    "        dt_obj = datetime.fromtimestamp(da)\n",
    "        print(dt_obj,da)\n",
    "     \n",
    "for t in tweets:\n",
    "    if t['id'] == 1615589959540551680:\n",
    "        print(dt.convertDate(t['created_at'])  )\n",
    "        \n",
    "times = None  \n",
    "id = None\n",
    "for i,t in enumerate(tweets):\n",
    "    if i == 0:\n",
    "        times = t['timestamp_ms']\n",
    "        id = t['id']\n",
    "    else:\n",
    "        if t['timestamp_ms'] < times:\n",
    "            times = t['timestamp_ms']\n",
    "print(times)\n",
    "da = (float(times)/1000.0)\n",
    "dt_obj = datetime.fromtimestamp(da)\n",
    "print(da, dt_obj, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DstTzInfo 'America/Merida' LMT-1 day, 18:02:00 STD>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytz\n",
    "pytz.timezone('America/Merida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time: 2006-02-24 18:00:00\n",
      "type of dt: <class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "  \n",
    "  \n",
    "timestamp = 1545730073\n",
    "dt_obj = datetime.fromtimestamp(1140825600)\n",
    "  \n",
    "print(\"date_time:\",dt_obj)\n",
    "print(\"type of dt:\",type(dt_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Gustavo A. Madero\n",
      "1 Gustavo A. Madero\n",
      "2 Cuauhtémoc\n",
      "3 Benito Juárez\n",
      "4 Xochimilco\n",
      "5 Gustavo A. Madero\n",
      "6 Iztacalco\n",
      "7 Cuauhtémoc\n",
      "8 Cuauhtémoc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for e,i in enumerate(tots['delegacion']):\n",
    "    print(e,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccSintomas = dict()\n",
    "diccSintomas['mentales'] = mentales\n",
    "diccSintomas['sintomas'] = sintomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mentales': ['salud mental', 'depresion', 'depresivo', 'cansado', 'cansancio', 'fatigado', 'fatiga', 'cansada', 'fatigada', 'insomnio', 'falta de sueño', 'enojado', 'enojada', 'irritable', 'irritante', 'irritado', 'irritada', 'poca hambre', 'sin hambre', 'sin apetito', 'poco apetito', 'frustacion', 'frustado', 'frustada', 'acomplejado', 'acomplejada', 'me cuesta concentrarme', 'sin concentracion', 'concentrado', 'concentrada', 'me siento inutil', 'inservible', 'ansiedad', 'ansioso', 'ansiosa', 'crisis de ansiedad', 'dolor de espalda', 'temblores', 'me late muy rapido el corazon', 'tension', 'nervioso', 'nerviosa', 'me agito rapido', 'me preocupo demasiado', 'me da miedo', 'miedo', 'miedosa', 'miedoso', 'miedito', 'miedillo', 'miedita', 'ansiosos', 'taquicardia', 'sueño', 'somnolencia', 'dormir', 'despertar', 'cama', 'cansancio', 'fatica', 'fatigo', 'fatigado', 'fatigada', 'fatigados', 'fatigadas', 'irritable', 'insoportable', 'soportable', 'insoportables', 'temblor', 'temblores', 'pasa algo', 'algo está pasando', 'sentimiento', 'llorar', 'lloré', 'lloró', 'lloramos', 'lloro', 'llore', 'lloremos', 'sensibilidad', 'hipersensible', 'hipersensibilidad', 'sentimental', 'chipioso', 'chipil', 'aberración', 'aberracion', 'aberrante', 'aberrantes', 'ablutomanía', 'abatimiento', 'abreacción', 'cerebral', 'cerebro', 'cerebelo', 'abstemio', 'abstinencia', 'síndrome', 'complejo', 'problema', 'abulia', 'hipobulia', 'aburrimiento', 'aburrido', 'aburrición', 'aburrision', 'aburricion', 'aburrirse', 'aburriendose', 'aburriendonos', 'sustancias', 'abuso', 'abusar', 'abusó', 'abusaron', 'animo', 'ánimo', 'estado de ánimo', 'bajo', 'dificultad para dormir', 'dificultad para conciliar', 'conciliar sueño', 'exceso de sueño', 'no me puedo parar de la cama', 'hambre', 'hambriento', 'apetito', 'perdida de peso', 'pérdida de peso', 'pérdida', 'peso', 'gordo', 'engordar', 'concentrarse', 'difícil concentrarser', 'concentrarme', 'concentración', 'lento', 'letargo', 'letargado', 'desesperanza', 'perder la esperanza', 'perdí esperanza', 'esperanza', 'abandono', 'abandonar', 'placer', 'pérdida de placer', 'me hacía feliz', 'me gustaba', 'nerviosismo', 'nervioso', 'agitado', 'agitación', 'tenso', 'tensado', 'tensión', 'peligro', 'inminente', 'pánico', 'paniqueado', 'paniqueada', 'catástrofe', 'respiración acelerada', 'hiperventilar', 'sudoración', 'sudor', 'sudé', 'sudo', 'temblores', 'debil', 'debilidad', 'problemas para concentrarse', 'concentrarse', 'concentrar', 'conciliar', 'problemas estomacáles', 'preocupaciones', 'evitar situaciones'], 'sintomas': ['dolor de cabeza', 'cuerpo cortado', 'tos', 'fiebre', 'dolor de espalda', 'dolor de garganta', 'cansancio', 'diarrea', 'cefalea', 'fiebre', 'temperatura', 'calentura', 'jaqueca', 'jaqueka', 'estornudo', 'hiperventilar', 'falta de aire', 'sin fuerza', 'debil', 'debilidad', 'irritacion', 'dolor de cuerpo', 'cuerpo cortado', 'fragil', 'fragilidad', 'frágil', 'fiebre', 'fiebroso', 'temperatura', 'tos', 'tos seca', 'tos rasposa', 'garganta', 'estómago', 'panaa', 'estomago', 'panza', 'pansa', 'cansancio', 'faatiga', 'fatigado', 'fatigada', 'adormecimiento', 'adormecido', 'olfato', 'conjuntivitis', 'ojos', 'garganta', 'diarrea', 'pérdida de gusto', 'erupciones', 'cutáneo', 'cutánea', 'cambios de color', 'dedos', 'pies', 'manos', 'garganta', 'gañote', 'saliva', 'pasar saliva', 'baba', 'intenso', 'dolor', 'dolores', 'pérdida de', 'perdida de', 'garganta reseca', 'ardor de ojos', 'ardor', 'hipertensión', 'hipertension', 'arterial', 'arteria', 'arterias', 'cardiaco', 'pulmonar', 'corazón', 'corazon', 'corason', 'pulmones', 'pulmón', 'pulmon', 'diabetes', 'diabetis', 'cáncer', 'cancer', 'respieren', 'respiro', 'respirar', 'opresión', 'pecho', 'dolor en el pecho', 'dificultad para', 'me dificulta']}\n"
     ]
    }
   ],
   "source": [
    "strdicc = str(diccSintomas)\n",
    "print(strdicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diccactual = dict(strdicc)\n",
    "strdicc = strdicc.replace(\"'\",'\"')\n",
    "#strdicc = json.loads(strdicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"mentales\": [\"salud mental\", \"depresion\", \"depresivo\", \"cansado\", \"cansancio\", \"fatigado\", \"fatiga\", \"cansada\", \"fatigada\", \"insomnio\", \"falta de sueño\", \"enojado\", \"enojada\", \"irritable\", \"irritante\", \"irritado\", \"irritada\", \"poca hambre\", \"sin hambre\", \"sin apetito\", \"poco apetito\", \"frustacion\", \"frustado\", \"frustada\", \"acomplejado\", \"acomplejada\", \"me cuesta concentrarme\", \"sin concentracion\", \"concentrado\", \"concentrada\", \"me siento inutil\", \"inservible\", \"ansiedad\", \"ansioso\", \"ansiosa\", \"crisis de ansiedad\", \"dolor de espalda\", \"temblores\", \"me late muy rapido el corazon\", \"tension\", \"nervioso\", \"nerviosa\", \"me agito rapido\", \"me preocupo demasiado\", \"me da miedo\", \"miedo\", \"miedosa\", \"miedoso\", \"miedito\", \"miedillo\", \"miedita\", \"ansiosos\", \"taquicardia\", \"sueño\", \"somnolencia\", \"dormir\", \"despertar\", \"cama\", \"cansancio\", \"fatica\", \"fatigo\", \"fatigado\", \"fatigada\", \"fatigados\", \"fatigadas\", \"irritable\", \"insoportable\", \"soportable\", \"insoportables\", \"temblor\", \"temblores\", \"pasa algo\", \"algo está pasando\", \"sentimiento\", \"llorar\", \"lloré\", \"lloró\", \"lloramos\", \"lloro\", \"llore\", \"lloremos\", \"sensibilidad\", \"hipersensible\", \"hipersensibilidad\", \"sentimental\", \"chipioso\", \"chipil\", \"aberración\", \"aberracion\", \"aberrante\", \"aberrantes\", \"ablutomanía\", \"abatimiento\", \"abreacción\", \"cerebral\", \"cerebro\", \"cerebelo\", \"abstemio\", \"abstinencia\", \"síndrome\", \"complejo\", \"problema\", \"abulia\", \"hipobulia\", \"aburrimiento\", \"aburrido\", \"aburrición\", \"aburrision\", \"aburricion\", \"aburrirse\", \"aburriendose\", \"aburriendonos\", \"sustancias\", \"abuso\", \"abusar\", \"abusó\", \"abusaron\", \"animo\", \"ánimo\", \"estado de ánimo\", \"bajo\", \"dificultad para dormir\", \"dificultad para conciliar\", \"conciliar sueño\", \"exceso de sueño\", \"no me puedo parar de la cama\", \"hambre\", \"hambriento\", \"apetito\", \"perdida de peso\", \"pérdida de peso\", \"pérdida\", \"peso\", \"gordo\", \"engordar\", \"concentrarse\", \"difícil concentrarser\", \"concentrarme\", \"concentración\", \"lento\", \"letargo\", \"letargado\", \"desesperanza\", \"perder la esperanza\", \"perdí esperanza\", \"esperanza\", \"abandono\", \"abandonar\", \"placer\", \"pérdida de placer\", \"me hacía feliz\", \"me gustaba\", \"nerviosismo\", \"nervioso\", \"agitado\", \"agitación\", \"tenso\", \"tensado\", \"tensión\", \"peligro\", \"inminente\", \"pánico\", \"paniqueado\", \"paniqueada\", \"catástrofe\", \"respiración acelerada\", \"hiperventilar\", \"sudoración\", \"sudor\", \"sudé\", \"sudo\", \"temblores\", \"debil\", \"debilidad\", \"problemas para concentrarse\", \"concentrarse\", \"concentrar\", \"conciliar\", \"problemas estomacáles\", \"preocupaciones\", \"evitar situaciones\"], \"sintomas\": [\"dolor de cabeza\", \"cuerpo cortado\", \"tos\", \"fiebre\", \"dolor de espalda\", \"dolor de garganta\", \"cansancio\", \"diarrea\", \"cefalea\", \"fiebre\", \"temperatura\", \"calentura\", \"jaqueca\", \"jaqueka\", \"estornudo\", \"hiperventilar\", \"falta de aire\", \"sin fuerza\", \"debil\", \"debilidad\", \"irritacion\", \"dolor de cuerpo\", \"cuerpo cortado\", \"fragil\", \"fragilidad\", \"frágil\", \"fiebre\", \"fiebroso\", \"temperatura\", \"tos\", \"tos seca\", \"tos rasposa\", \"garganta\", \"estómago\", \"panaa\", \"estomago\", \"panza\", \"pansa\", \"cansancio\", \"faatiga\", \"fatigado\", \"fatigada\", \"adormecimiento\", \"adormecido\", \"olfato\", \"conjuntivitis\", \"ojos\", \"garganta\", \"diarrea\", \"pérdida de gusto\", \"erupciones\", \"cutáneo\", \"cutánea\", \"cambios de color\", \"dedos\", \"pies\", \"manos\", \"garganta\", \"gañote\", \"saliva\", \"pasar saliva\", \"baba\", \"intenso\", \"dolor\", \"dolores\", \"pérdida de\", \"perdida de\", \"garganta reseca\", \"ardor de ojos\", \"ardor\", \"hipertensión\", \"hipertension\", \"arterial\", \"arteria\", \"arterias\", \"cardiaco\", \"pulmonar\", \"corazón\", \"corazon\", \"corason\", \"pulmones\", \"pulmón\", \"pulmon\", \"diabetes\", \"diabetis\", \"cáncer\", \"cancer\", \"respieren\", \"respiro\", \"respirar\", \"opresión\", \"pecho\", \"dolor en el pecho\", \"dificultad para\", \"me dificulta\"]}'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = json.loads(db.getAnalisisDicc('sintomas'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT diccionario_tema FROM temas  WHERE tema LIKE 'COVID';\"\n",
    "ndic = db.selectOne(sql) #para modificar cuando se convierta a class\n",
    "filtro = dataImport(ndic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fechas_milisegundos(fecha_inicial, fecha_final):\n",
    "    \"\"\"recibe fechas en formato YYYY-mm-dd y las regresa en milisegundos, la fecha final la regresa a las 0:00 horas i.e. es al comienzo del dia\"\"\"\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    fechaInicial = datetime.strptime(fecha_inicial, formato_fecha)\n",
    "    #milliseconds1 = int(round(fechaInicial.timestamp() * 1000))-18000000 #se restan los milisegundos a 5 horas para coincidir con la zona horaria que traen los tweets \n",
    "    milliseconds1 = int(round(fechaInicial.timestamp() * 1000))#- 21600000 #se restan los milisegundos a 6 horas para coincidir con la zona horaria que traen los tweets \n",
    "    \n",
    "    fechaFinal = datetime.strptime(fecha_final, formato_fecha)\n",
    "    #milliseconds2 = int(round(fechaFinal.timestamp() * 1000))-18000000\n",
    "    milliseconds2 = int(round(fechaFinal.timestamp() * 1000))#-21600000\n",
    "    return milliseconds1,milliseconds2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1674021600000, 1674108000000)\n",
      "2023-01-18 1674021600.0\n"
     ]
    }
   ],
   "source": [
    "formato_fecha = \"%Y-%m-%d\"\n",
    "\n",
    "a, h =fechas_ayer_hoy_string()\n",
    "\n",
    "m, n = fechas_milisegundos(a,h)\n",
    "print(fechas_milisegundos(a,h))\n",
    "da = (float(m)/1000.0)\n",
    "dt_obj = datetime.fromtimestamp(da)\n",
    "print(dt_obj.strftime(formato_fecha),da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = consulta_ayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: es_core_news_sm in /home/adrian/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Collecting spacy<3.2.0,>=3.1.0\n",
      "  Using cached spacy-3.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Using cached thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (3.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (0.4.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (5.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (1.23.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (59.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (1.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (0.6.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (3.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (0.7.9)\n",
      "Requirement already satisfied: jinja2 in /home/adrian/.local/lib/python3.10/site-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/adrian/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->es_core_news_sm) (4.3.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->es_core_news_sm) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.0.1)\n",
      "Installing collected packages: thinc, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.6\n",
      "    Uninstalling thinc-8.1.6:\n",
      "      Successfully uninstalled thinc-8.1.6\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.0\n",
      "    Uninstalling spacy-3.4.0:\n",
      "      Successfully uninstalled spacy-3.4.0\n",
      "Successfully installed spacy-3.1.7 thinc-8.0.17\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable}  -m pip install es_core_news_sm\n",
    "\n",
    "# python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pip3` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install  es_core_news_sm==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3781834535.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [28]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(   spacy.)\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(   spacy.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40cc23766191e3511d28625666db93fd1c83d99b65ccfacac35180954be4e323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
